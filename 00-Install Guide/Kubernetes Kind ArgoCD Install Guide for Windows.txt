
INSTALL AND USE ARGOCD STACK
============================

We need to install evironment tools depending on what kind of setup we will use for managing Kubernetes clusters.

We will work with Power Shell CLI for each terminal command included in the instructions.

We will go true installing lightweight setup on Windows 11 with:
	1. Docker - container management application
	2. Kind - application that uses Docker to manage Kubernetes clusters with containers
	3. Kubectl - Kubernetes CLI 
	4. Install Terraform
	5. Helm - Kubernetes extension that help managing manifest files in Kuberentes clusters
	6. Create Kuberentes Cluster with Kind
	7. install ArgoCD
	8. Install Argo Rollouts and Kargo CLI
	9. Create second Kubernetes Kind cluster
	10. Adding the second clusters to ArgoCD
	11. Install Argo Workflows on main cluster
	12. Install Argo Workflows CLI on Windows

Example CI/CD pipeline configuration with GitHub
	13. Create and configure working namespace
	14. Install Nexus private image registry
	15. Install MinIO artifacts storage with Helm

Example CI/CD pipeline configuration with GitLab
	16. Install GitLab and set local repositories
	17. Deploy Argo SetUp
	18. Add inline analysis
	19. Backup Docker Desktop Kind clustr

Example CI/CD pipeline configuration with GitHub
	20. Install Nexus private image registry
	21. Install MinIO artifacts storage with Helm
	22. INstall NGROK
	23. ArgoCD Setup with GitHub
	24. Install ESO (External Secret Operator) with Helm




1. Istall Docker on Windows
===========================
Register, download and install Docker - https://www.docker.com/products/docker-desktop/


2. Install Kind on Windows
==========================

Kind install documentation - https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries

Install Kind with Shell
	terminal --> curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.30.0/kind-windows-amd64
Move-Item .\kind-windows-amd64.exe c:\Users\your_user\kind.exe

Verify Kind installation
	terminal --> kind version


3. Install kubectl on Windows
=============================
Kubernetes documentation for installing kubectl
	- https://kubernetes.io/docs/tasks/tools/

Install kubectl on Windows
	- https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/

Install kubectl using curl
	terminal --> curl.exe -LO "https://dl.k8s.io/release/v1.34.0/bin/windows/amd64/kubectl.exe"

Verify installation using power shell
	power shell --> $(Get-FileHash -Algorithm SHA256 .\kubectl.exe).Hash -eq $(Get-Content .\kubectl.exe.sha256)
	# result: true

Find the location of the kubectl.exe file - usually in C:/Users/your_user and copy the path from the windows explorer navigation bar.

Append or prepend the kubectl binary folder to your PATH environment variable.
	- WinKey, type 'environment' and click on 'Edit the system variables', click 'Environment Variables' button at the bottom
	- In the System variables list, find 'Path', mark it and click 'Edit...' button. Click 'New' and paste the path to the kubectl.exe file in the field. Click 'Ok' button. 
	# If we have more than one kubectl installed we need to 'move up' the path of the specific installation so Windows will use the first found installation as default.
	- Restart the PC

Open CMD and check if the kubectl location and installation are successful
	terminal --> where kubectl		# this should list all kubectl installations and their locations
	terminal --> kubectl version --client	# this should list all client and their versions (kubectl, kustomize etc.)

	# example result:
	Client Version: v1.34.0
	Kustomize Version: v5.7.1


4. Install Helm on Windows
==========================

Helm install documentation - https://helm.sh/docs/intro/install

Download Helm - https://get.helm.sh/helm-v4.0.1-windows-amd64.zip

Extract helm.exe in c:\Users\your_user\		# same as Kind

Verify Helm installation
	terminal --> helm version


5. Install Terraform
====================

Terraform download page - https://developer.hashicorp.com/terraform/install

Choose version AMD64 and download it. Unzip it in C:/Users/your-user/ or C:/Program Files/terraform.
	- copy the file path

Add the path to the binary in the PATH environment variable
	- open 'View advanced system settings/Control Panel/System Properties/' and click on [Environment Variables]
	- click on the "Path" variable and on the [Edit] button
	- press the [New] button and add the path to where the terraform.exe file is
	- click consecutively on [OK] buttons to exit all settings.
	- confirm installation with terminal --> terraform -v



6. Create Kuberentes Cluster with Kind
======================================

In case we want cluster with more than one node:

1. Create kind-config.yaml file on you system. Example - C:\Users\your_user\Kubernetes_Kind_Projects\kind-config.yaml


Option 1 - using helm official chart for ArgoCD

kind-config.yaml
-------------------------------------------------
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 32073
    hostPort: 32073
    protocol: TCP
  - containerPort: 32074
    hostPort: 32074
    protocol: TCP
- role: worker
- role: worker
-------------------------------------------------



Option 2 - Use Nginx ingress controller

kind-config-nginx.yaml
-------------------------------------------------
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 32073
    protocol: TCP
  - containerPort: 443
    hostPort: 32074
    protocol: TCP
- role: worker
- role: worker
-------------------------------------------------

We will use nginx ingresscontroller - option 2

2. Create cluster 
	terminal --> kind create cluster --config kind-config-nginx.yaml --image kindest/node:v1.34.0

In case we want simple one node cluster we start it with:
	terminal --> kind create cluster --name my-argocd-cluster --image kindest/node:v1.34.0

3. Confirm cluster creation
	terminal --> kubectl cluster-info 
	terminal --> kubectl get nodes

4. Set roles for worker nodes
	terminal --> kubectl label nodes kind-worker kind-worker2 node-role.kubernetes.io/worker= --overwrite

	# result:
	node/kind-worker labeled
	node/kind-worker2 labeled




SET ALIAS FOR KUBECTL (Shell and CMD)
-------------------------------------


for SHELL:

We use command 'k' instead of 'kubectl' to save time and prevent mistyping.

Edit Shell Profile configs
	terminal --> if (!(Test-Path -Path $PROFILE)) { New-Item -ItemType File -Path $PROFILE -Force }
notepad $PROFILE
	or
	terminal --> notepad $PROFILE
	or
	code $PROFILE


---------------------------------
# Alias for kubectl
Set-Alias -Name k -Value kubectl
---------------------------------
Save changes and restart PowerShell


For CMD:
Set alias for kubectl
	etrminal --> doskey k=kubectl $*


PREVENT SCHEDULING PODS ON CONTROLPLANE
---------------------------------------
We need to set taints on the controlplain to prevent application pod deployment on it. By default is set be we need to be sure.

Check existing taints
	shell terminal --> kubectl describe node kind-control-plane | Select-String -Pattern "Taint"

	# result: Taints:             node-role.kubernetes.io/control-plane:NoSchedule


If the result is different set the taints:

Prevent scheduling deployments on the controlplane
	terminal --> kubectl taint nodes kind-control-plane node-role.kubernetes.io/control-plane:NoSchedule --overwrite


RENAME LOCAL CLUSTER
--------------------

To deploy application on the local cluster using labels we need to create secret matching the matching label filter using ArgoCD UI or CLI. This way we can set labels to the local cluster.

Create local cluster secret:

local-secret.yaml
-----------------------------------------------
apiVersion: v1
kind: Secret
metadata:
  name: local
  labels:
    argocd.argoproj.io/auto-label-cluster-info: "true"  # auto label the local cluster with kubernetes cluster version
    argocd.argoproj.io/secret-type: cluster		# set secret label type cluster
    environment: pre-staging				# set environment label
type: Opaque
stringData:
  name: local                                # name of the local cluster - will be renamed automatically to this one if different
  server: https://kubernetes.default.svc		# set local cluster address
  config: |
    {
      "tlsClientConfig": {
        "insecure": false
      }
    }
-----------------------------------------------

!!! The local cluster will be autolabeled with the kuberletes cluster version. !!!

Create the secret in the cluster
	terminal --> k apply -f local-secret.yaml

	# result: secret/local created






7. INSTALL ARGOCD WITH HELM
===========================

We will go over 2 types of Helm ArgoCD installations
	I. 	Modifying Helm Chart and overwriting argocd server-service to NodePort - values.yaml
	II. 	Modifying Helm Chart and Nginx ingress-controller and overwriting ingress controller - values-nginx.yaml
	RECOMMENDED SECOND WAY !


I. Modifying Helm Chart and overwriting argocd server-service to NodePort - values.yaml:
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
1. Add ArgoCD repository in Helm registry
	terminal --> helm repo add argo https://argoproj.github.io/argo-helm 

	# result: "argo" has been added to your repositories

	1.1. Confirm repo is added
		terminal --> helm repo list

		# result:
		NAME    URL
		argo    https://argoproj.github.io/argo-helm

2. Update Helm repositories
	terminal --> helm repo update
	
	# result: 
	Hang tight while we grab the latest from your chart repositories...
	...Successfully got an update from the "argo" chart repository
	Update Complete. ⎈Happy Helming!⎈


3. Create argocd namespace
	terminal --> kubectl create ns argocd

	# result: namespace/argocd created

We need to configure NodePort service for ArgoCD UI to be accessable:
4. Create values.yaml for ArgoCD installation
	terminal --> notepad values.yaml

values.yaml
---------------------------------
server:
  service:
    type: NodePort
    nodePortHttp: 32073
    nodePortHttps: 32074
---------------------------------


4. Install ArgoCD on the cluster
	terminal --> helm install argocd argo/argo-cd -f .\values.yaml -n argocd --create-namespace

	# helm install					- use helm application to install
	# argocd 	 				- name of the helm deployment
	# argo/argo-cd					- helm chart
	# -f .\values.yaml				- use file values.yaml in the current dir
	# -n argocd --create-namespace			- selected namespace or create it


5. List service in argocd namespace and check the type of the server service
	terminal --> k get svc -n argocd

	# result:
NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
argocd-applicationset-controller   ClusterIP   10.96.186.86    <none>        7000/TCP                     93s
argocd-dex-server                  ClusterIP   10.96.69.196    <none>        5556/TCP,5557/TCP            93s
argocd-redis                       ClusterIP   10.96.155.85    <none>        6379/TCP                     93s
argocd-repo-server                 ClusterIP   10.96.236.248   <none>        8081/TCP                     93s
argocd-server                      NodePort    10.96.187.167   <none>        80:32073/TCP,443:32074/TCP   93s	# NodePort





II. Modifying Helm Chart and Nginx ingress-controller and overwriting ingress controller - values-nginx.yaml
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

Create configuration file from the original Helm Argocd chart
	terminal --> helm show values argo/argo-cd > values-nginx.yaml

Open the file and search for 'ingress'. On line 2390 we can find the ingress fields. 

Modify ingress configurations as official documentation - https://argo-cd.readthedocs.io/en/latest/operator-manual/ingress/#kubernetesingress-nginx

Option 1 - https://argo-cd.readthedocs.io/en/latest/operator-manual/ingress/#option-1-ssl-passthrough

We clear the file and set final configuration as follow:

values-nginx-simplified.yaml
--------------------------------------------------------------------
server:
    # Argo CD server ingress configuration
    ingress:
        # -- Enable an ingress resource for the Argo CD server
        enabled: true
        annotations:
            nginx.ingress.kubernetes.io/ssl-passthrough: "true"

        # -- Defines which ingress controller will implement the resource
        ingressClassName: nginx

        # -- Argo CD server hostname
        # @default -- `""` (defaults to global.domain)
        hostname: "argocd.localhost"

        # -- The path to Argo CD server
        path: /

        # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`
        pathType: Prefix

        # -- Enable TLS configuration for the hostname defined at `server.ingress.hostname`
        ## TLS certificate will be retrieved from a TLS secret `argocd-server-tls`
        ## You can create this secret via `certificate` or `certificateSecret` option
        tls: true
--------------------------------------------------------------------
# save as UTF-8 format


If we install with this method we need to uninstall the installed version first

Uninstall existing ArgoCD 
	terminal --> helm uninstall argocd -n argocd
	
	# result:
	These resources were kept due to the resource policy:
	[CustomResourceDefinition] applications.argoproj.io
	[CustomResourceDefinition] applicationsets.argoproj.io
	[CustomResourceDefinition] appprojects.argoproj.io
	release "argocd" uninstalled


Before installing ArgoCD with nginx ingress controller we need to install nginx deployment
	terminal --> kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.0/deploy/static/provider/kind/deploy.yaml

	Wait 2-3 minutes and confirm that Nginx deployment is running
		terminal --> k get pods -n ingress-nginx

		# result:
		NAME                                        READY   STATUS      RESTARTS   AGE
		ingress-nginx-admission-create-2clvc        0/1     Completed   0          49s
		ingress-nginx-admission-patch-xwb6r         0/1     Completed   0          49s
		ingress-nginx-controller-569c5c4774-ftqzm   1/1     Running     0          49s		# running

		terminal --> k get deployment -n ingress-nginx

		# result:
		NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
		ingress-nginx-controller   1/1     1            1           55s
		

	Edit the deployment and add '--enable-ssl-passthrough' flag in specs
		terminal --> k edit deployment ingress-nginx-controller -n ingress-nginx

------------------------------------------------------------
...
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        - --watch-ingress-without-class=true
        - --enable-metrics=false
        - --publish-status-address=localhost
        - --enable-ssl-passthrough						# added
...
------------------------------------------------------------
save changes

Install ArgoCD with nginx ingress controller
	terminal --> helm install argocd argo/argo-cd -f values-nginx-simplified.yaml -n argocd --create-namespace


Confirm ArgoCD deployment
	terminal --> k get ingress -n argocd

	# result:
	NAME            CLASS   HOSTS       ADDRESS     PORTS     AGE
	argocd-server   nginx   localhost   localhost   80, 443   2m8s		# Wait ADDRESS to appear

Set default  namespace in the main cluster
	terminal --> kubectl config set-context kind-kind --namespace=argocd

	# result: Context "kind-kind" modified.

Find the password of admin user of ArgoCD
	powershell terminal --> k -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }

	# result: password

Connect to the Node with ArgoCD CLI
	terminal --> argocd login argocd.localhost:32074 --insecure --grpc-web --username admin --password <password>
	or
	terminal --> argocd login argocd.localhost:32074 --insecure --grpc-web --username admin
	terminal --> password

Confirm default namespace config
	terminal --> argocd cluster list


ACCESS ARGOCD UI
----------------

Add host address to Windows host list
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 argocd.localhost'
		- save the file and exit

Find initial password for ArgoCD:
	shell terminal --> k -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }

	# result: password we will use to access the ArgoCD UI

Open ArgoCD UI:

In case we are using official Helm chart:
- Open braowser and go to https://argocd.localhost:32074
- Press Advanced and accept the risk and continue

In case we are using nginx ingress controller
- Open braowser and go to localhost			# the hostname we set up in the values-nginx.yaml file
- Press Advanced and accept the risk and continue

Login to ArgoCD
	Username: admin
	Password: use the password


List clusters
	terminal --> argocd cluster list

	# result:
	SERVER                               NAME      VERSION  STATUS      MESSAGE  PROJECT
	https://cluster2-control-plane:6443  external  1.34     Successful
	https://kubernetes.default.svc       local     1.34     Successful 			# renamed
	
	# the local sluter is renamed from 'in-cluster' to 'local'




8. Install Argo Rollouts and Kargo CLI
======================================

Installation documentation - https://argo-rollouts.readthedocs.io/en/stable/installation/


Step 1:
-------
install.yaml - Standard installation method.
	Create argo-rollouts namespace
		terminal --> kubectl create namespace argo-rollouts

		# result: namespace/argo-rollouts created

	Apply argo-rollouts manifest into argo-rollouts namespace
		terminal --> kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

		# result:
		customresourcedefinition.apiextensions.k8s.io/analysisruns.argoproj.io created
		customresourcedefinition.apiextensions.k8s.io/analysistemplates.argoproj.io created
		customresourcedefinition.apiextensions.k8s.io/clusteranalysistemplates.argoproj.io created
		customresourcedefinition.apiextensions.k8s.io/experiments.argoproj.io created
		customresourcedefinition.apiextensions.k8s.io/rollouts.argoproj.io created
		serviceaccount/argo-rollouts created
		clusterrole.rbac.authorization.k8s.io/argo-rollouts created
		clusterrole.rbac.authorization.k8s.io/argo-rollouts-aggregate-to-admin created
		clusterrole.rbac.authorization.k8s.io/argo-rollouts-aggregate-to-edit created
		clusterrole.rbac.authorization.k8s.io/argo-rollouts-aggregate-to-view created
		clusterrolebinding.rbac.authorization.k8s.io/argo-rollouts created
		configmap/argo-rollouts-config created
		secret/argo-rollouts-notification-secret created
		service/argo-rollouts-metrics created
		deployment.apps/argo-rollouts created


	Confirm components installtion
		terminal --> kubectl get all -n argo-rollouts


		# result:
		NAME                                 READY   STATUS    RESTARTS   AGE
		pod/argo-rollouts-65c8945cc7-zchlm   1/1     Running   0          4m40s

		NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
		service/argo-rollouts-metrics   ClusterIP   10.96.238.38   <none>        8090/TCP   4m40s

		NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/argo-rollouts   1/1     1            1           4m40s

		NAME                                       DESIRED   CURRENT   READY   AGE
		replicaset.apps/argo-rollouts-65c8945cc7   1         1         1       4m40s


Step 2:
-------

Install kargo dashboard
	terminal --> kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/dashboard-install.yaml

	# result:
	serviceaccount/argo-rollouts-dashboard created
	clusterrole.rbac.authorization.k8s.io/argo-rollouts-dashboard created
	clusterrolebinding.rbac.authorization.k8s.io/argo-rollouts-dashboard created
	service/argo-rollouts-dashboard created
	deployment.apps/argo-rollouts-dashboard created

	(Optional - just for test)
	Foreward port for kargo dashboard
		terminal --> kubectl port-forward -n argo-rollouts svc/argo-rollouts-dashboard 3100:3100
		terminal --> Ctrl+C


Set additional service dashboard-ingress.yaml for dashboard to access it.

dashboard-ingress.yaml
-----------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argo-rollouts-dashboard
  namespace: argo-rollouts
spec:
  ingressClassName: nginx
  rules:
  - host: rollouts.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: argo-rollouts-dashboard
            port:
              number: 3100
-----------------------------------------------------

Create the service for Argo Rollouts
	terminal --> kubectl apply -f dashboard-ingress.yaml -n argo-rollouts

	# result: ingress.networking.k8s.io/argo-rollouts-dashboard created



Confirm components installtion
	terminal --> kubectl get all -n argo-rollouts


	# result:
	NAME                                           READY   STATUS    RESTARTS   AGE
	pod/argo-rollouts-65c8945cc7-6f4c5             1/1     Running   0          3m12s
	pod/argo-rollouts-dashboard-5659ccf55b-vdrwg   1/1     Running   0          2m5s
	
	NAME                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
	service/argo-rollouts-dashboard   ClusterIP   10.96.71.204   <none>        3100/TCP   2m5s
	service/argo-rollouts-metrics     ClusterIP   10.96.70.150   <none>        8090/TCP   3m12s

	NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/argo-rollouts             1/1     1            1           3m13s
	deployment.apps/argo-rollouts-dashboard   1/1     1            1           2m6s

	NAME                                                 DESIRED   CURRENT   READY   AGE
	replicaset.apps/argo-rollouts-65c8945cc7             1         1         1       3m13s
	replicaset.apps/argo-rollouts-dashboard-5659ccf55b   1         1         1       2m6s


Add host address to Windows host list
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 rollouts.localhost'
		- save the file and exit


STEP 3 
------

Download kargo-dashboards binary for Windows
	terminal --> iwr https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-windows-amd64 -OutFile kubectl-argo-rollouts.exe

Create folder
terminal --> mkdir ~/bin -Force
Move-Item ./kubectl-argo-rollouts.exe ~/bin/kubectl-argo-rollouts.exe

Add the binary to PATH
	terminal --> [Environment]::SetEnvironmentVariable("Path", $env:Path + ";$env:USERPROFILE\bin", "User")


Set Alias for kubectl-argo-rollouts:
------------------------------------
Open Power Shell profile AS ADMIN and set alias below. 
	terminal --> code $PROFILE

-----------------------------------------------------
# Alias for kubectl
Set-Alias -Name k -Value kubectl

# Alias for kubectl-argo-rollouts
Set-Alias -Name kargo -Value kubectl-argo-rollouts	# added
-----------------------------------------------------

Restart power shell 
Load Power Shell profile
	terminal --> . $PROFILE

Verify binary installation
	termimnal --> kubectl-argo-rollouts version
	or
	terminal --> kargo version

Open Kargo Dashboard on http://rollouts.localhost:32073/
      			--------------------------------


IMPORTANT
=========

EXPLOANATION ABOUT DOCKER KUBERNETES KIND CLUSTERS COMMUNICATION

Our main (local) cluster is serving ArgoCD and Kargo Dashboard. Its configuration is as follow:

kind-config-nginx.yaml
-------------------------------------------------
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 32073				# this is HTTP
    protocol: TCP
  - containerPort: 443
    hostPort: 32074				# this is HTTPS
    protocol: TCP
- role: worker
- role: worker
-------------------------------------------------

ArgoCD requires TLS/HTTPS communication and we access it on port 32074 - https://argocd.localhost:32074/applications
Kargo Dashboard requires HTTP communication and we access it on port 32073 - http://rollouts.localhost:32073/rollouts/

All configured host addresses in C:\Windows\System32\drivers\etc\hosts connected with our cluster application are accessed on port 32073 or 32074 (for local cluster) depending of on which tool we are looking at them (ArgoCD or Kargo) with.

127.0.0.1 argocd.localhost			# this use HTTPS - 32074
127.0.0.1 rollouts.localhost			# this use HTTP - 32073




9. Create second Kubernetes Kind cluster
========================================

Set cluster configuration kind-config-nginx-2.yaml

kind-config-nginx-2.yaml
-----------------------------------------------
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  apiServerAddress: "127.0.0.1"
  apiServerPort: 6444  # Different port from first cluster (default is 6443)
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 32075  # Different from first cluster (32073)
    protocol: TCP
  - containerPort: 443
    hostPort: 32076  # Different from first cluster (32074)
    protocol: TCP
- role: worker
- role: worker
-----------------------------------------------


Create the cluster
	terminal --> kind create cluster --name cluster2 --config kind-config-nginx-2.yaml --image kindest/node:v1.34.0

Find the context of the new cluster
	terminal --> kubectl config get-contexts

	# result:
	CURRENT   NAME            CLUSTER         AUTHINFO        NAMESPACE
	          kind-cluster2   kind-cluster2   kind-cluster2			# target context
        *  	  kind-kind       kind-kind       kind-kind       argocd

Use 'kind-cluster2' context to add the new cluster to ArgoCD Server

Set the context of the cluster2 to kubectl
	terminal --> kubectl config use-context kind-cluster2

Find the names of the containers of the cluster 2
	terminal --> docker ps | findstr cluster2

# result:
3258b4633e7e   kindest/node:v1.34.0   "/usr/local/bin/entr???"   34 minutes ago   Up 34 minutes                                                                              cluster2-worker
862e5199ab8a   kindest/node:v1.34.0   "/usr/local/bin/entr???"   34 minutes ago   Up 34 minutes                                                                              cluster2-worker2
b6fd04b32bce   kindest/node:v1.34.0   "/usr/local/bin/entr???"   34 minutes ago   Up 34 minutes   0.0.0.0:32075->80/tcp, 0.0.0.0:32076->443/tcp, 127.0.0.1:6444->6443/tcp    cluster2-control-plane

Find the port of the controlplane of the cluster2 - cluster2-control-plane
	terminal --> docker inspect cluster2-control-plane --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'

	# result: 172.18.0.6

We will use this name "https://cluster2-control-plane:6443" or IP address ("https://172.18.0.6:6443") to configure the second cluster into ArgoCD Server. The name is recommended because the IP address can be changed.


Login to ArgoCD CLI to cluster 1:
---------------------------------

We have installed ArgoCD on the first cluster so we need to login to it to configure the connection to the second cluster.

Set the context to kubectl with the cluster with installed ArgoCD server on it
	terminal --> kubectl config use-context kind-kind

Find the password of admin user of ArgoCD
	powershell terminal --> k -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }

	# result: password

Connect to the Node with ArgoCD CLI
	terminal --> argocd login localhost:32074 --insecure --grpc-web --username admin --password Tip-kdHFNljqdnq3
	or
	terminal --> argocd login localhost:32074 --insecure --grpc-web --username admin
	terminal --> password




10. Adding the second clusters to ArgoCD
========================================


Adding Cluster Manually
-----------------------

List clusters
	terminal --> argocd cluster list

	# result
	SERVER                          NAME        VERSION  STATUS      MESSAGE  PROJECT
	https://kubernetes.default.svc  in-cluster  1.34     Successful

We can also check in ArgoCD UI/Settings/Clusters

We have only the local Kind cluster we created from the start of the course.

Adding another cluster:
-----------------------
First we need to create a service account that will be responsible for the applications to the newly added cluster. It must have the necessary access to this cluster - admin.

Create cluster service account
	terminal --> k create sa new-cluster-sa

	# result: serviceaccount/new-cluster-sa created

Print cluster role admin
	terminal --> k get clusterrole cluster-admin

	# result:
	NAME            CREATED AT
	cluster-admin   2025-12-10T11:36:54Z


Show clusterrolebinding help commands
	terminal --> k create clusterrolebinding --help

Examples:
  # Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role
  kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1


Bind cluster-admin role to this new-cluster-sa by creating clusterrolebinding
	terminal --> k create clusterrolebinding argocd-clusterbinding --clusterrole=cluster-admin --serviceaccount=default:new-cluster-sa 

		# k						- kubectl common command
		# create					- action	
		# clusterrolebinding				- target object
		# argocd-clusterbinding				- name of the target object
		# --clusterrole=cluster-admin			- type of the target object
		# --serviceaccount=default:new-cluster-sa	- used existing role as default


	# result: clusterrolebinding.rbac.authorization.k8s.io/argocd-clusterbinding created

This new serviceaccount now has admin permissions on the cluster.

Test the permissions of the serviceaccount
	terminal --> k auth can-i create pods --as system:serviceaccount:default:new-cluster-sa 
	terminal --> k auth can-i delete pods --as system:serviceaccount:default:new-cluster-sa 
	terminal --> k auth can-i create deploy --as system:serviceaccount:default:new-cluster-sa 
	terminal --> k auth can-i delete deploy --as system:serviceaccount:default:new-cluster-sa 

	# result: yes

Manually geberate token for our new service account
	terminal --> k create token new-cluster-sa 

	# result: token


Show details for the new cluster
	terminal --> cat ~/.kube/config

	# we have line "server: https://127.0.0.1:58857"		# this is the new cluster IP address

Show certidicate of our new cluster
# Linux/Mac
	- Get the certificate directly from kubeconfig 
		terminal --> kubectl config view --raw -o jsonpath='{.clusters[0].cluster.certificate-authority-data}'

	- Decode it to see the actual cert
		terminal --> kubectl config view --raw -o jsonpath='{.clusters[0].cluster.certificate-authority-data}' | base64 -d

# Kind cluster
	- Get the CA cert from Kind
		terminal --> docker cp kind-control-plane:/etc/kubernetes/pki/ca.crt ./ca.crt

	- Then base64 encode it 
		# Linux/Mac
		terminal --> base64 -w 0 ca.crt
		# Windows PowerShell:
		terminal --> [Convert]::ToBase64String([IO.File]::ReadAllBytes("ca.crt"))


Set cluster secret manifest file called cluster-secret.yaml

cluster-secret.yaml
-----------------------------------------------
apiVersion: v1
kind: Secret
metadata:
  name: external-cluster
  labels:
    argocd.argoproj.io/secret-type: cluster			# important - seceret with type 'cluster'
type: Opaque
stringData:
  name: https://cluster2-control-plane:6443			# name of the new clustr when is added to ArgoCD API Server
  server: API_SERVER_ADDRESS		# set the new cluster IP address
  config: |
    {
      "bearerToken": "SERVICE_ACCOUNT_TOKEN",		# set the token we generated
      "tlsClientConfig": {
        "insecure": false,
        "caData": "BASE64 ENCODED CERTIFICATE"		# set the encoded cluster cetificate
      }
    }
-----------------------------------------------


Apply the secret
	terminal --> k apply -f cluster-secret.yaml

	# secret/external-cluster created


List clusters
	terminal --> argocd cluster list

	# result
	SERVER                          		NAME        V		     ERSION  STATUS      MESSAGE  PROJECT
	https://cluster2-control-plane:6443		https://cluster2-control-plane:6443	     Unknown	 Cluster...
	https://kubernetes.default.svc  		in-cluster  		     1.34     Successful




Adding cluster using terraform
------------------------------

We will connect new cluster using terrafomr and make all steps we made manually with IaC.

We have 4 terraform files
	- providers.tf
	- main.tf
	- variables.tf
	- terraform.tfvars


Hashicorp Kubernetes provider - https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs

ArgoCD terrafomr provider - https://registry.terraform.io/providers/argoproj-labs/argocd/latest/docs

ArgoCD cluster resource - https://registry.terraform.io/providers/argoproj-labs/argocd/latest/docs/resources/cluster


providers.tf
-----------------------------------------------
terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "3.0.1"
    }
    argocd = {
      source = "argoproj-labs/argocd"
      version = "7.12.4"
    }
  }
}

provider "kubernetes" {
  config_path    = var.config_path
  config_context = var.config_context
}

provider "argocd" {
  alias       = "argocd_server"
  server_addr = var.server_addr
  username    = var.username
  password    = var.password
  insecure    = var.insecure
}
-----------------------------------------------




kubernetes_service_account_v1 - https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/service_account_v1

kubernetes_secret_v1 - https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/secret_v1

kubernetes_cluster_role_binding_v1 - https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/cluster_role_binding_v1

argocd_cluster - https://registry.terraform.io/providers/argoproj-labs/argocd/latest/docs/resources/cluster


main.tf
-----------------------------------------------
resource "kubernetes_service_account_v1" "argocd_manager_sa" {		# create kubernetes service account
  metadata {
    name      = var.sa_name		# service account name
    namespace = var.sa_namespace	# service account namespace - default
  }
}

resource "kubernetes_secret_v1" "argocd_manager_secret" {	# create kubernetes secret to create and set a token for our sa
  metadata {
    name = var.argocd-manager-secret					# secret name
    annotations = {
      "kubernetes.io/service-account.name" = "${kubernetes_service_account_v1.argocd_manager_sa.metadata.0.name}"
    }
  }
  type = "kubernetes.io/service-account-token"	# generate token for the "kubernetes_service_account_v1" "argocd_manager_sa"
}

resource "kubernetes_cluster_role_binding_v1" "argocd_manager_binding" {  # create cluster role binding to bind service account to
  metadata {								# cluster role called cluster admin
    name = var.argocd-manager-binding		# set cluster role binding name
  }

  role_ref {
    api_group = "rbac.authorization.k8s.io"	# role based access control api group
    kind      = "ClusterRole"			# set the kind of the binding
    name      = "cluster-admin"			# set the role we want to bind with the binding
  }

  subject {
    kind      = "ServiceAccount"
    name      = kubernetes_service_account_v1.argocd_manager_sa.metadata.0.name		# ref to the sa name
    namespace = kubernetes_service_account_v1.argocd_manager_sa.metadata.0.namespace	# ref to the sa namespace
  }
}

data "kubernetes_secret_v1" "argocd_manager_secret" {		# retrieve external resporce data
  metadata {
    name = kubernetes_secret_v1.argocd_manager_secret.metadata.0.name	# take data from the external cluster config
  }
}

resource "argocd_cluster" "new_cluster" {	# ArgoCD cluster
  provider = argocd.argocd_server		# alias of our current ArgoCD server - "lcaolhost:32074"
  server = var.new_cluster_server_addr		# new cluster IP address
  name     = "external"				# name the external cluster

  config {	# token from kubernetes_service_account_v1's (argocd_manager_sa) kubernetes_secret_v1 (argocd_manager_secret)
    bearer_token = "${lookup(data.kubernetes_secret_v1.argocd_manager_secret.data, "token")}"

    tls_client_config {		# use new cluster certificate
      ca_data  = "${lookup(data.kubernetes_secret_v1.argocd_manager_secret.data, "ca.crt")}"
      insecure = false		# set secure connection 
    }
  }
}
-----------------------------------------------



variables.tf
-----------------------------------------------
variable "server_addr" {
  type        = string
  description = "The server address"
}

variable "username" {
  type        = string
  description = "The Username"
}

variable "password" {
  type        = string
  description = "The Password"
}

variable "insecure" {
  type        = bool
  description = "The Connection Insecure flag"
}

variable "sa_name" {
  type        = string
  description = "The name of argocd serviceaccount"
}

variable "sa_namespace" {
  type        = string
  description = "The namespace of argocd serviceaccount"
}

variable "config_path" {
  type        = string
  description = "The path of kube config"
}

variable "config_context" {
  type        = string
  description = "The context in kube config"
}

variable "argocd-manager-binding" {
  type        = string
  description = "The name of argocd manager clusterrole binding"
}

variable "argocd-manager-secret" {
  type        = string
  description = "The name of argocd manager secret"
}

variable "argocd_secret_cluster" {
  type        = string
  description = "The name of argocd secret cluster"
}

variable "secret_labels" {
  type        = map(string)
  description = "The Labels of argocd manager secret"
}

variable "new_cluster" {
  type        = string
  description = "The name of new cluster"
}

variable "new_cluster_server_addr" {
  type        = string
  description = "The name of new cluster server address"
}
-----------------------------------------------


terraform.tfvars
-----------------------------------------------
server_addr            = "argocd.localhost:32074"	# ArgoCD host connection address and port
username               = "admin"			# ArgoCD username
password               = "password"			# ArgoCD admin password
insecure               = true
sa_name                = "argocd-manager-sa"		# set new cluster's service account name
sa_namespace           = "default"
config_path            = "~/.kube/config"		# cluster config path
config_context         = "kind-cluster2"		# new cluster context (as what user, in what cluster and namespace)
argocd-manager-binding = "argocd-manager-binding"	# new cluster admin binding
argocd-manager-secret  = "argocd-manager-secret"	# new cluster admin role secret
argocd_secret_cluster  = "argocd-secret-cluster"	# new cluster secret
secret_labels = {
  "argocd.argoproj.io/secret-type" = "cluster"		# type of the resource
}
new_cluster             = "cluster2"			# new cluster name
new_cluster_server_addr = "https://cluster2-control-plane:6443"	# set the new cluster container name and its port
-----------------------------------------------

Format terrafomr files
	terminal --> terraform fmt

Download and install providers configurations
	terminal --> terraform init		# result :Terraform has been successfully initialized!

Validate terraform files
	terminal --> terraform validate		# result: Success! The configuration is valid.

Plan terraform resources
	terminal --> terraform plan		# result: Plan: 4 to add, 0 to change, 0 to destroy.

Apply terraform resources
	terminal --> terraform apply
	terminal --> yes

	# result: Apply complete! Resources: 4 added, 0 changed, 0 destroyed.


List contexts
	terminal --> kubectl config get-contexts


Switch to local cluster context
	terminal --> kubectl config use-context kind-kind


List service account
	terminal --> k get sa

# result:
NAME                               SECRETS   AGE
argocd-manager-sa		   0	     20s		# this is our new service account
argocd-application-controller      0         11d
argocd-applicationset-controller   0         11d
argocd-dex-server                  0         11d
argocd-notifications-controller    0         11d
argocd-redis-secret-init           0         11d
argocd-repo-server                 0         11d
argocd-server                      0         11d
default                            0         11d
new-cluster-sa                     0         15h


List secrets
	terminal --> k get secret -n argocd

NAME                                 TYPE                 			DATA   AGE
argocd-manager-secret	 	     Kubernetes.io/service-account-token	3      35s	# this is our new secret
argocd-initial-admin-secret          Opaque               			1      11d
argocd-notifications-secret          Opaque               			0      11d
argocd-redis                         Opaque               			1      11d
argocd-secret                        Opaque               			5      11d
https-private-repo-secret            Opaque               			4      4d
https-private-repo-secret-template   Opaque               			4      3d22h
sh.helm.release.v1.argocd.v1         helm.sh/release.v1   			1      11d
ssh-private-repo-secret              Opaque               			3      3d23h
ssh-private-repo-secret-template     Opaque               			3      3d21h


List clusters
	terminal --> argocd cluster list

# result
SERVER                               NAME      VERSION  STATUS   MESSAGE                                                  PROJECT
https://cluster2-control-plane:6443  external           Unknown  Cluster has no applications and is not being monitored.
https://kubernetes.default.svc       local              Unknown  Cluster has no applications and is not being monitored.

When we deploy application the status will be 'Successful'



11. Install Argo Workflows on main cluster
==========================================

INSTALL ARGO WORKFLOWS WITH HELM
--------------------------------

This is the official repo for Argo Workflows Helm chart 
	- https://github.com/argoproj/argo-helm/tree/main/charts/argo-workflows

We are going to use ingress controller so we need to modify values.yaml file used with the chart.
	- Open the values.yaml - https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/values.yaml
	- Search for 'server' and on line 514 we can find the section. On line 686 we can find 'ingress' subsection.
		- The 'ingress' controller functionality is disabled by default


We will overwrite needed fields for this section in our local argo-workflows-values.yaml file and use it to deploy the Argo Workflow Chart.

We also add field 'extraArgs:' to manage authentication mode from '--auth-mode=client' (default) to '--auth-mode=server'. 
This will avoid requirements for bearer authentication token when we connect to Argo Workflows.
We can also overwrite and use 'server/NodePort' section for external access (not in this case).


argo-workflows-values.yaml
-------------------------------------------------
server:
  # -- Extra arguments to provide to the Argo server binary.
  ## Ref: https://argo-workflows.readthedocs.io/en/stable/argo-server/#options
  extraArgs:
    - --auth-mode=server
  
  # Argo Workflows server ingress configuration
  ingress:
    # -- Enable an ingress resource for the Argo Workflows server
    enabled: true

    # -- Defines which ingress controller will implement the resource
    ingressClassName: nginx

    # -- Argo Workflows server hostname
    hostname: "argo-workflows.localhost"
-------------------------------------------------


Create 'argo' namespace in our cluster
	terminal --> k create ns argo

	# result: namespace/argo created

Add Argo repository to our system
	terminal --> helm repo add argo https://argoproj.github.io/argo-helm

Confirm repo addition
	terminal --> helm repo list

	# result:
	NAME    URL
	argo    https://argoproj.github.io/argo-helm


Deploy Argo Workflows Helm Chart
	terminal --> helm install my-workflow argo/argo-workflows -n argo -f argo-workflows-values.yaml

	# helm 					- common helm command
	# install				- action
	# my-workflow				- object name
	# argo/argo-workflows			- used chart
	# -n argo				- destination namespace
	# -f argo-workflows-values.yaml		- use argo-workflows-values.yaml custom values

	# result:
	NAME: my-workflow
	LAST DEPLOYED: Thu Jan  8 14:02:07 2026
	NAMESPACE: argo
	STATUS: deployed
	REVISION: 1
	DESCRIPTION: Install complete
	TEST SUITE: None


List resources deployed in 'argo' namespace
	terminal --> k get all -n argo

	# result:
	NAME                                                                  READY   STATUS    RESTARTS   AGE
	pod/my-workflow-argo-workflows-server-58dd98f67-pnj6f                 1/1     Running   0          16m
	pod/my-workflow-argo-workflows-workflow-controller-58864d5898-vvspv   1/1     Running   0          16m

	NAME                                        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
	service/my-workflow-argo-workflows-server   ClusterIP   10.96.23.4   <none>        2746/TCP   16m

	NAME                                                             READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/my-workflow-argo-workflows-server                1/1     1            1           16m
	deployment.apps/my-workflow-argo-workflows-workflow-controller   1/1     1            1           16m

	NAME                                                                        DESIRED   CURRENT   READY   AGE
	replicaset.apps/my-workflow-argo-workflows-server-58dd98f67                 1         1         1       16m
	replicaset.apps/my-workflow-argo-workflows-workflow-controller-58864d5898   1         1         1       16m


Create rolebinding for admin role in 'argo' namespace so we can execute actions with Argo Workflows UI
	terminal --> k create rolebinding default-admin --clusterrole=admin --serviceaccount=argo:default -n argo

	# result: rolebinding.rbac.authorization.k8s.io/default-admin created


Add host address to Windows host list
	- Open PowerShell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 argo-workflows.localhost'
		- save the file and exit

Add host address to Linux host list
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 argo-workflows.localhost'
	- save changes and exit - escape, :wq!, enter


We can now access our Argo Workflows UI on https://argo-workflows.localhost:32074/






INSTALL ARGO WORKFLOW MANUALLY
------------------------------

We need to uninstall any other Argo Workflow installation to proceed with this instruction.

Create 'argo' namespace for Argo Workflows
	terminal --> k create ns argo

	# result: namespace/argo created

Deploy Argo Workflows manifests
	terminal --> kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/latest/download/install.yaml

	# result:
	customresourcedefinition.apiextensions.k8s.io/clusterworkflowtemplates.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/cronworkflows.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/workflowartifactgctasks.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/workfloweventbindings.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/workflowtaskresults.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/workflowtasksets.argoproj.io created
	customresourcedefinition.apiextensions.k8s.io/workflowtemplates.argoproj.io created
	serviceaccount/argo created
	serviceaccount/argo-server created
	role.rbac.authorization.k8s.io/argo-role created
	clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-admin created
	clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-edit created
	clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-view created
	clusterrole.rbac.authorization.k8s.io/argo-cluster-role created
	clusterrole.rbac.authorization.k8s.io/argo-server-cluster-role created
	rolebinding.rbac.authorization.k8s.io/argo-binding created
	clusterrolebinding.rbac.authorization.k8s.io/argo-binding created
	clusterrolebinding.rbac.authorization.k8s.io/argo-server-binding created
	configmap/workflow-controller-configmap created
	service/argo-server created
	priorityclass.scheduling.k8s.io/workflow-controller created
	deployment.apps/argo-server created
	deployment.apps/workflow-controller created


Confirm successfull installation
	terminal --> k get all -n argo

	# result:
	NAME                                      READY   STATUS    RESTARTS   AGE
	pod/argo-server-744f7588b8-tnqh4          1/1     Running   0          52s
	pod/workflow-controller-6c84fcfb6-d2f9l   1/1     Running   0          52s

	NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
	service/argo-server   ClusterIP   10.96.232.144   <none>        2746/TCP   52s

	NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
	deployment.apps/argo-server           1/1     1            1           52s
	deployment.apps/workflow-controller   1/1     1            1           52s

	NAME                                            DESIRED   CURRENT   READY   AGE
	replicaset.apps/argo-server-744f7588b8          1         1         1       52s
	replicaset.apps/workflow-controller-6c84fcfb6   1         1         1       52s


Create rolebinding for admin role for installation namespace
	terminal --> k create rolebinding argo-server-admin --clusterrole=admin --serviceaccount=argo:argo-server -n argo
	

Set ingress controller for permanent access without port forwarding for test purposes

argo-workflows-ingress.yaml
-------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argo-workflows
  namespace: argo
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:
  - host: argo-workflows.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: argo-server
            port:
              number: 2746
-------------------------------------------------

Deploy the argo-workflows-ingress.yaml controller
	terminal --> k apply -f argo-workflows-ingress.yaml -n argo

	# result: ingress.networking.k8s.io/argo-workflows created


Patch the argo workflow server to avoid authentication (NOT FOR PRODUCTION !)
	shell terminal --> kubectl patch deployment argo-server -n argo --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/args", "value": ["server", "--auth-mode=server"]}]'

	# result: deployment.apps/argo-server patched


Add host address to Windows host list
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 argo-workflows.localhost'
		- save the file and exit

Add host address to Windows host list on Linux
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 argo-workflows.localhost'
	- save changes and exit - escape, :wq!, enter


We can now access our Argo Worflows UI on https://argo-workflows.localhost:32074/




12. Install Argo Workflows CLI on Windows with Shell
====================================================

1. Download latest version from the official repository:
	- Go to official Argo CLI repo - and download latest stable version - https://github.com/argoproj/argo-workflows/releases
	- Extract it in Downloads folder (you'll get argo-windows-amd64 file)
	- Move the file to C://Users/username					# fix the username

2. Rename the file 
	terminal --> Rename-Item -Path "C:\Users\usernemae\argo-windows-amd64.exe" -NewName "argo.exe"	# fix the username


2. Add to PATH with PowerShell
	- Run PowerShell as Administrator, fix your username path and execute the command
		terminal --> [Environment]::SetEnvironmentVariable("Path", [Environment]::GetEnvironmentVariable("Path", "User") + ";C:\Users\username", "User")

	# fix the folder path ";C:\Users\username\bin" - set correct username

3. Restrat shell and verify Argo CLI installation
	terminal --> argo version

	# result:
	argo: v4.0.0-rc2
  	BuildDate: 2025-12-22T16:07:52Z
  	GitCommit: 5f7c7ec6f5f6d6589a2c0cbc20acd79ca078f76f
  	GitTreeState: clean
  	GitTag: v4.0.0-rc2
  	GoVersion: go1.24.11
  	Compiler: gc
  	Platform: windows/amd64




13. Create and configure working namespace
==========================================

Create working namespace 'argo-events'
	terminal --> kubectl create ns argo-events

	# result: argo-events/workflows created


IF ARGO WORKFLOWS IN INSTALLED WITH HELM
----------------------------------------

Scenario 1 - Working with the default user account of the working namespace
----------

Give workflow-controller permissions to act with pods in the working namespace:
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:my-workflow-argo-workflows-workflow-controller -n argo-events

Give permissions to the working namespace's default service account:
	terminal --> k create rolebinding default-admin --clusterrole=admin --serviceaccount=argo-events:default -n argo-events



Scenario 2 - Create separate service account for the working namespace
----------

Give workflow-controller permissions to act with pods in the working namespace:
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:my-workflow-argo-workflows-workflow-controller -n argo-events

Create service account in working 'argo-events' namespace
	terminal --> kubectl create serviceaccount argo-workflows -n argo-events

Create rolebinding for the created service account and give it admin rights
	terminal --> kubectl create rolebinding argo-workflows-admin --clusterrole=admin --serviceaccount=argo-events:argo-workflows -n argo-events



IF ARGO WORKFLOWS IN INSTALLED MANUALLY
---------------------------------------

Scenario 1 - Working with the default service account
----------
Give workflow-controller permissions (NOTE: different service account name)
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:argo -n argo-events

Give permissions to working namespace's default service account
	terminal --> kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=argo-events:default -n argo-events


Scenario 2 - Create separate service account
----------
Give workflow-controller permissions (NOTE: different service account name)
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:argo -n argo-events

Create service account
	terminal --> kubectl create serviceaccount argo-workflows -n argo-events

Give it permissions
	terminal --> kubectl create rolebinding argo-workflows-admin --clusterrole=admin --serviceaccount=argo-events:argo-workflows -n argo-events







14. Install Nexus private image registry
========================================

1. Download and install Java from: https://www.oracle.com/java/technologies/downloads/#jdk25-windows

	Confirm java installation
		terminal --> java -version

2 Download Nexus Repository OSS (Open Source) for Windows - https://www.sonatype.com/products/nexus-community-edition-download

3. Install Nexus
	- unarchive on the PC and navigate to the folder
	- Open shell as administrator and run
		terminal --> .\install-nexus-service.bat

4. Start Nexus service
	terminal --> net start SonatypeNexusRepository

	Check if port 8081 is listening
		terminal --> netstat -ano | findstr :8081

5. Login to Nexus on http://localhost:8081/#login

	Find the initial generated credentials in the installed directory 'sonatype-work\nexus3\admin.password' or with shell
		terminal --> type E:\Installed\nexus-3.88.0-08-win-x86_64\sonatype-work\nexus3\admin.password

	Login to the app and finish the installtion. Relogin.
		Username: admin
		Password: admin123		(default)


Create Nexus repository - example usage
---------------------------------------
- Open Nexus - http://localhost:8081
- Go to Settings/Repositories/Create Repository/docker(hosted)
	- Name: argo-demo
	- Other Connectors
		- HTTP: 8085
	- Docker Registry API Support
		Select Checkbox "Allow clients to use the V1 API to interact with this repository"
	- Create Repository

- Go to Settings/Security/Realms
	- Set Docker Bearer Token Realm to Active
	- Save

 

Configure Docker Desktop to communicate with the created repository
	- Open Docker/Settings/Docker Engine
	- Add
	-------------------------------------------------
	{
	  "insecure-registries": ["localhost:8085"]
	}
	-------------------------------------------------
	- Apply and Restart



Test Docker connection with Nexus by pushing the image created earlier
	Login to Nesus true the configured address
		terminal --> docker login host.docker.internal:8085
		terminal --> admin
		terminal --> nexsus_password

	Retag (rename) and push the image we created earlier to the Nexus repository
		terminal --> docker tag nginx:v1 host.docker.internal:8085/argo-demo/nginx:alpine
		terminal --> docker push host.docker.internal:8085/argo-demo/nginx:alpine

	The image should be visualized in the Nexus repository
		- Go to http://localhost:8081/#browse/browse
		- Then navigate to v2/argo-demo/nginx/tags




Configure Argo Workflows to pull/push images from/to Nexus
----------------------------------------------------------

We need to allow Argo Workflows communication in the Docker Engine Settings to be able to access Nexus platofrm. In this case we are using Docker Desktop and we add the internal Docker host address and the port of the Nexus repository.

Configure Argo Workflows to communicate with Nexus repository
	- Open Docker/Settings/Docker Engine
	- Add
	-------------------------------------------------
	{
	  "insecure-registries": ["localhost:8085", "host.docker.internal:8085"]
	}
	-------------------------------------------------
	- Apply and Restart



Configure Argo Workflows to pull images from Nexus
--------------------------------------------------


Create containerd daemonset configuration

containerd-config-daemonset.yaml
-------------------------------------------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: containerd-registry-config
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: containerd-registry-config
  template:
    metadata:
      labels:
        name: containerd-registry-config
    spec:
      hostPID: true
      hostNetwork: true
      initContainers:
      - name: configure-containerd
        image: alpine:latest
        command:
        - sh
        - -c
        - |
          set -e
          
          # Check if configuration already exists
          if grep -q "host.docker.internal:8085" /host/etc/containerd/config.toml 2>/dev/null; then
            echo "Registry configuration already exists, skipping..."
            exit 0
          fi
          
          # Append registry configuration
          cat >> /host/etc/containerd/config.toml << 'EOF'

          [plugins."io.containerd.grpc.v1.cri".registry.mirrors."host.docker.internal:8085"]
            endpoint = ["http://host.docker.internal:8085"]
          [plugins."io.containerd.grpc.v1.cri".registry.configs."host.docker.internal:8085".tls]
            insecure_skip_verify = true
          EOF
          
          echo "Configuration added successfully"
          
          # Find and restart containerd process
          CONTAINERD_PID=$(nsenter -t 1 -m -u -i -n -p pgrep containerd | head -n 1)
          if [ -n "$CONTAINERD_PID" ]; then
            echo "Sending SIGHUP to containerd (PID: $CONTAINERD_PID)"
            nsenter -t 1 -m -u -i -n -p kill -HUP $CONTAINERD_PID
            sleep 2
            echo "Containerd reloaded"
          else
            echo "Warning: Could not find containerd process"
          fi
        securityContext:
          privileged: true
        volumeMounts:
        - name: containerd-config
          mountPath: /host/etc/containerd
      containers:
      - name: pause
        image: registry.k8s.io/pause:3.9
        resources:
          requests:
            cpu: 1m
            memory: 4Mi
      volumes:
      - name: containerd-config
        hostPath:
          path: /etc/containerd
          type: Directory
      tolerations:
      - operator: Exists
-------------------------------------------------

Apply the deamonset
	terminal --> kubectl apply -f containerd-config-daemonset.yaml

	# result: daemonset.apps/containerd-registry-config created

Test nodes communication
	terminal --> k get nodes

Wait 2 minutes and delete the daemonset
	terminal --> kubectl delete -f containerd-config-daemonset.yaml



Give Argo Workflows access to Nexus
-----------------------------------

For this task we need to create Docker secret called 'docker-config-secret' in our Argo Workflows working 'argo-events' namespace. We have to mount our docker credentials to this sescret.

We need to create docker-config.json file and set the Nexus credentials so the workflow can pull and push images.

1. Encode the nexus creadentials with shell
	terminal --> $auth = [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes("admin:admin123"))
$auth
	result: YWRtaW46YWRtaW4xMjM=

2. Create docker-config.json
-----------------------------------------
{
  "auths": {
    "host.docker.internal:8085": {
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
-----------------------------------------

From the folder location create Docker secret
	terminal --> kubectl create secret generic docker-config-secret --from-file=config.json=./docker-config.json -n argo-events

	# result: secret/docker-config-secret created




15. Install MinIO artifacts storage with Helm
=============================================

MinIO shared storage service that can be used by workflows in any namespace. It is commonly used module that is installed in 'argo' namespace as Argo Workflows and should be separated from the workload namespaces in our cluster.

Prerequisites
	- kubectl CLI tool
	- Helm CLI tool

MinIO shared storage service that can be used by workflows in any namespace. It is commonly used module that is installed in 'argo' namespace as Argo Workflows and should be separated from the workload namespaces in our cluster.

INSTALL MINIO WIHT HELM
-----------------------
1. Install MinIO helm repository on our PC
	terminal --> helm repo add minio https://charts.min.io/
	terminal --> helm repo update


2. Install MinIO chart
	terminal --> helm install argo-artifacts minio/minio --set resources.requests.memory=512Mi --set replicas=1 --set persistence.enabled=false --set mode=standalone --set rootUser=admin --set rootPassword=password123 --set buckets[0].name=my-bucket --set buckets[0].policy=none --set buckets[0].purge=false -n argo


3. Create credentials secret in working namespace
	terminal --> k create secret generic my-minio-cred --from-literal=access-key=admin --from-literal=secret-key=password123 -n argo-events

	# result: secret/my-minio-cred created


4. Set Ingress controller for MinIO to be externally accessed 

minio-ingress.yaml
-------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minio-console-ingress
  namespace: argo
spec:
  ingressClassName: nginx
  rules:
  - host: minio.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: argo-artifacts-minio-console
            port:
              number: 9001
-------------------------------------------------

Apply the ingress manifest
	terminal --> k apply -f minio-ingress.yaml -n argo

	# result: ingress.networking.k8s.io/minio-console-ingress created

	
Add host address to Windows host list on Windows
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 minio.localhost'
		- save the file and exit

Add host address to Windows host list on Linux
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 minio.localhost'
	- save changes and exit - escape, :wq!, enter


ACCESS MINIO
------------

Decode the username (rootUser) with shell
	terminal --> kubectl get secret argo-artifacts-minio -n argo -o jsonpath='{.data.rootUser}' | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([Convert]::FromBase64String($_.Trim())) }

	# result: admin

Decode the password (rootPassword) with shell
	terminal --> kubectl get secret argo-artifacts-minio -n argo -o jsonpath='{.data.rootPassword}' | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([Convert]::FromBase64String($_.Trim())) }

	# result: password123


Access the MinIO app on http://minio.localhost:32073/login
	- admin
	- password123

We can create bucket if we haven't with the installation command.
Create a bucket named 'my-bucket'. We will store our artifacts in this bucket.


USE MINIO
---------
To use artifacts we need to configure MinIO repository to store the artifacts from the working namespace. MinIO repository is configured in minio-artifact-repo-cm.yaml configmap file below.

minio-artifact-repo-cm.yam
-------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  # If you want to use this config map by default, name it "artifact-repositories". Otherwise, you can provide a reference to a
  # different config map in `artifactRepositoryRef.configMap` in the workflow.
  name: artifact-repository           # this name is not the default name
  annotations:
    # v3.0 and after - if you want to use a specific key, put that key into this annotation.
    workflows.argoproj.io/default-artifact-repository: minio-artifact-repo
data:
  minio-artifact-repo: |
    s3:
      bucket: my-bucket
      endpoint: argo-artifacts-minio.argo:9000
      insecure: true
      accessKeySecret:
        name: my-minio-cred
        key: access-key 
      secretKeySecret:
        name: my-minio-cred
        key: secret-key
-------------------------------------------------

Create the configmap into the working namespace
	terminal --> kubectl apply -f minio-artifact-repo-cm.yaml -n argo-events

	# result: configmap/artifact-repository created

Confirm ConfigMap creation
	terminal --> kubectl get configmap artifact-repository -n <namespace>

	# result:
	NAME                  DATA   AGE
	artifact-repository   1      104m





GENERATE GITHUB CREDENTIALS
---------------------------

We need to provide GitHub credentials to our workflow to access the application repository and manage it - clone it, make changes to the manifests etc.

Login to GitHub - https://github.com/
Go to https://github.com/settings/tokens
	- Generate New Token (classic)
		- Name: argo-workflows
		- Scope
			- repo - Full control of private repositories 
			- workflow - Update GitHub Action workflows (Optional)
		- Generate Token
	- Copy the value (save it safe temporary)

Create local environment variables with GitHub Username and Token
	terminal --> $GITHUB_USERNAME = "git_username"			# set your username
	terminal --> $GitHubTokenName = "argo-workflows"		# set the token name
	terminal --> $GITHUB_TOKEN = "generated_token"			# set the token

Test the creation of the environment cariables
	terminal --> echo $GITHUB_USERNAME
	terminal --> echo $GitHubTokenName
	terminal --> echo $GITHUB_TOKEN


Create secretfor workflow task in the next step in the same terminal session else the envs will be deleted and the access will be denied.


We will use the environment variables to create secrets objects in our Kubernetes cluster and set them in the workflow.

CREATE SECRETS FOR GITHUB CREDENTAILS IN THE SAME SHELL SESSION
---------------------------------------------------------------

Create secret in our working namespace to use GitHub credentials safetly
	terminal --> kubectl create secret generic github-credentials --from-literal=username=$GITHUB_USERNAME --from-literal=token_name=$GitHubTokenName --from-literal=token=$GITHUB_TOKEN -n argo-events
	
	# result: secret/github-credentials created





16. Install GitLab and set local repositories
=============================================


Install local GitLab repository with Docker 
-------------------------------------------

Create volumes on your Windows PC
	terminal --> mkdir D:\Docker\gitlab\config
	terminal --> mkdir D:\Docker\gitlab\logs
	terminal --> mkdir D:\Docker\gitlab\data

Start Docker container with GitLab with exact version (if we use latest we can update the version and lose the configs)
	terminal --> docker run -d --hostname gitlab.local --name gitlab -p 80:80 -p 443:443 -p 22:22 --restart always -v D:\Docker\gitlab\config:/etc/gitlab -v D:\Docker\gitlab\logs:/var/log/gitlab -v D:\Docker\gitlab\data:/var/opt/gitlab gitlab/gitlab-ce:18.8.1-ce.0


Set new password
----------------

Connecto the gitlab container
	terminal --> docker exec -it gitlab bash

Run the GitLab Rails console
		terminal --> gitlab-rails console -e production

Set the new password adn save it
		terminal --> user = User.find_by(username: 'root')
		terminal --> user.password = 'YourStrongPassword123!'
		terminal --> user.password_confirmation = 'YourStrongPassword123!'
		terminal --> user.save!

		# result: => true

Exit the console and container
	terminal --> exit
	terminal --> exit

One line shell command (change the password)
	terminal --> docker exec -it gitlab gitlab-rails runner "user = User.find_by(username: 'root'); user.password = 'YourStrongPassword123!'; user.password_confirmation = 'YourStrongPassword123!'; user.save!"

	
Add host address to Windows host list on Windows
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 gitlab.local'
		- save the file and exit

Add host address to Windows host list on Linux
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 gitlab.local'
	- save changes and exit - escape, :wq!, enter



Login to GitLab on http://gitlab.localhost/users/sign_in
	Username: root
	Password: your_password

Create a blank project
	- name: argo-config
	- Project URL: http://gitlab.localhost/root/argo-config
	- Create project

Creaet another project for our app
	- name: my-app
	- Project URL: http://gitlab.localhost/root/my-app
	- Create project



TEST CLUSTER <--> GITLAB CONNECTION
-----------------------------------

Our cluster must be connected with our GitLab server. In this case we are running both via Docker containers

Find our GitLab IP address
	terminal --> docker network inspect bridge | Select-String "Gateway"

	# result: 172.17.0.1

Test connections between cluster and GitLab
	Try to reach Kind cluster from GitLab container
		terminal --> docker exec -it gitlab curl -v http://172.17.0.1:32073
	Try to reach GitLab container from Kind cluster
		terminal --> docker exec -it kind-control-plane curl -v http://172.17.0.1

	# if successfull we will set this IPs for connection between gitlab and kind cluster


GENERATE GITLAB ACCESS TOKEN
----------------------------

Create User Personal Token to connect the 'argo-config' repository to ArgoCD
	- connect to GitLab container
		terminal --> docker exec -it gitlab gitlab-rails console

	- Find the admin user 
		terminal --> user = User.find_by(username: 'root')


	- create personal token
		terminal --> 
token = user.personal_access_tokens.create(
  name: 'gitlab-full-access',
  scopes: ['api', 'read_repository', 'write_repository'],
  expires_at: 1.year.from_now
)
	
	- Display the token - COPYAND SAVE THE TOKEN IMMEDIATELY!
		terminal --> 
puts "=" * 60
puts "TOKEN: #{token.token}"
puts "=" * 60

	- Verify creation
		terminal --> 
if token.persisted?
  puts "✅ Token created successfully!"
  puts "Name: #{token.name}"
  puts "Scopes: #{token.scopes}"
else
  puts "❌ Error: #{token.errors.full_messages}"
end

	- Exit
		terminal --> exit	


Create access token for argo-config project
	- go tp my-app repo/ Settings/Access tokens/Add new token
		- Token name: argo-config-token
		- Expiration date: No Expiration Date
		- Select a role: Maintainer
			- check 'api', 'read_repository' and 'write_repository' option
		- Create project access token

Create access token for my-app project
	- go tp my-app repo/ Settings/Access tokens/Add new token
		- Token name: my-app-token
		- Expiration date: No Expiration Date
		- Select a role: Maintainer
			- check 'api', 'read_repository' and 'write_repository' option
		- Create project access token

We can use different tokens for the different projects (my-app and argo-config) We can use one user access token - not a good practice.

Since the user is the same we use one env var in both secrets.


ALLOW LOCAL HOOKS
-----------------
Go to GitLab/Admin/Settings/Network/Outbound requests/
	- check 'Allow requests to the local network from webhooks and integrations'
	- check 'Allow requests to the local network from system hooks'
	- in the Local IP addresses and domain names ... add
		172.17.0.1
		127.0.0.1
		argo.events
	- Save Changes



CREATE ENV VARIABLES WITH GITLAB CREDENTIALS 
--------------------------------------------
Create local environment variables with GitLab Username and Tokens
	terminal --> $GITLAB_USERNAME = "root"				# set your username
	terminal --> $GITLAB_USER_TOKEN = "generated_user_token"	# set your user token
	terminal --> $GITLAB_CONFIG_TOKEN = "generated_config_token"	# set the config token
	terminal --> $GITLAB_APP_TOKEN = "generated_app_token"		# set the app token

Test the creation of the environment cariables
	terminal --> echo $GITLAB_USERNAME
	terminal --> echo $GITLAB_USER_TOKEN
	terminal --> echo $GITLAB_APP_TOKEN
	terminal --> echo $GITLAB_CONFIG_TOKEN


CREATE SECRETS FOR GITLAB CREDENTAILS
-------------------------------------
Create secret in 'argo-events' namespace to use GitLab credentials for creating the webhook in my-app project
	terminal --> kubectl create secret generic app-repo-credentials --from-literal=username=$GITLAB_USERNAME --from-literal=token=$GITLAB_APP_TOKEN -n argo-events

	# result: secret/app-repo-credentials created

Create secret in 'argo-events' namespace to use GitLab credentials for managing used image in the roolout in argo-config project
	terminal --> kubectl create secret generic config-repo-credentials --from-literal=username=$GITLAB_USERNAME --from-literal=token=$GITLAB_CONFIG_TOKEN -n argo-events

	# result: secret/github-credentials created

We use this secret in event source and in the sersor manifests.



GIVE ARGOCD ACCESS TO GITLAB REPO
---------------------------------

Create secret for GitLab repository access
	terminal --> kubectl create secret generic argocd-gitlab-repo --from-literal=username=root --from-literal=password=$GITLAB_USER_TOKEN --from-literal=url=http://172.17.0.1/root/argo-config.git -n argocd

Label it so ArgoCD recognizes it as a repository credential
	terminal --> kubectl label secret argocd-gitlab-repo argocd.argoproj.io/secret-type=repository -n argocd

Check if the repository is added successfully in ArgoCD UI - https://argocd.localhost:32074/settings/repos



GIVE ARGO WORKFLOWS ACCESS TO NEXUS
-----------------------------------

For this task we need to create Docker secret called 'docker-config-secret' in our Argo Workflows working 'argo-events' namespace. We have to mount our docker credentials to this sescret.

We need to create docker-config.json file and set the Nexus credentials so the workflow can pull and push images.

1. Encode the nexus creadentials with shell
	terminal --> $auth = [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes("admin:admin123"))
$auth
	result: YWRtaW46YWRtaW4xMjM=

2. Create docker-config.json
-----------------------------------------
{
  "auths": {
    "host.docker.internal:8085": {
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
-----------------------------------------

From the folder location create Docker secret
	terminal --> kubectl create secret generic docker-config-secret --from-file=config.json=./docker-config.json -n argo-events

	# result: secret/docker-config-secret created






Create Locally repositories and connect them with GitLab
--------------------------------------------------------

Create argo-demo folder and clone the 2 folders from the repo:

1. my-app
---------
1.1. Clone the empty repo we created
	terminal --> git clone http://gitlab.local/root/my-app.git

1.2. Set the files in the repo folder
	Navigate to folder
		terminal --> cd my-app

	Set the files
	|--my-app
	   |-- Dockerfile
	   |-- index.html

1.3. Push the fils
	terminal --> git add .
	terimnal --> git commit -m 'init repo'
	terminal --> git push
	terminal --> Username: root
	terminal --> Password: gitlab_password



2. argo-config
--------------
2.1. Clone the empty repo we created
	terminal --> git clone http://gitlab.local/root/argo-config.git

2.2. Set the files in the repo folder
	Navigate to folder
		terminal --> cd argo-config

	Set the files
	|--argo-cd
	|  |-- argo-app.yaml
	|
	|--argo-events
	|  |-- event-source.yaml
	|  |-- sensor.yaml
	|
	|--argo-rollouts
	   |-- nginx-ingress.yaml
	   |-- nginx-rollouts.yaml
	   |-- webhook-ingress.yaml

2.3. Push the fils
	terminal --> git add .
	terimnal --> git commit -m 'init repo'
	terminal --> git push






17. Deploy Argo SetUp
=====================


argo-events
   |-- event-source.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: EventSource				# event source obejct
metadata:
  name: ci
  namespace: argo-events
spec:
  service:
    ports:
      - port: 12000
        targetPort: 12000
  gitlab:                      # event source type
    argo-demo:                 # event name - we can hevae multiple events - only one in this example
      projects:
        - "1"                  # project ID in the GitLba repo - gitlab/my-app/settings/general
      webhook:                           # webhook section
        endpoint: /push                  # endpoint
        port: "12000"                    # port
        url: http://host.docker.internal:32073     # custom url that must be reachable from iside our gitlab server !!!
        # url: http://172.17.0.1:32073 if GitLab is running outside Docker - match the GitLab IP address
        # port 32073 is the default Kind cluster HTTP port 
      events:
        - PushEvents              # when we nmake a push event to gitlab it will be detected
      accessToken:
        key: token                      # References the 'token' key
        name: app-repo-credentials          # References the secret gitlab-credentials
      enableSSLVerification: false          # disable SSL verification
      gitlabBaseURL: http://172.17.0.1      # our local gitlab url found via 
      deleteHookOnFinish: true              # the webhook will be deleted when this event source is deleted
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argo-events-ingress
  namespace: argo-events
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /push
        pathType: Exact                 # Used to match the path exactly for Docker internal network
        backend:
          service:
            name: ci-eventsource-svc
            port:
              number: 12000

# Use if the ingress is connecting out of Docker network
# apiVersion: networking.k8s.io/v1
# kind: Ingress                                  # ingress
# metadata:
#   name: argo-demo-ingress
#   namespace: argo-events
# spec:
#   ingressClassName: nginx
#   rules:
#   - host: argo.events                         # host name of the ingress 
#     http:
#       paths:
#       - path: /
#         pathType: Prefix
#         backend:
#           service:
#             name: ci-eventsource-svc          # the backend service name created by the event source
#             port:
#               number: 12000
-------------------------------------------------

Add host address to Windows host list on Windows
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 argo.events'
		- save the file and exit

Add host address to Windows host list on Linux
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 argo.events'
	- save changes and exit - escape, :wq!, enter

We can test our the webhook endpoint on http://argo.events:32073




argo-events
   |-- sensor.yaml
-------------------------------------------------
# Create a service account with RBAC settings to allow the sensor to trigger workflows, and allow workflows to function.
# kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/rbac/sensor-rbac.yaml
# kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/rbac/workflow-rbac.yaml
apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: ci
  namespace: argo-events
spec:
  template:
    serviceAccountName: operate-workflow-sa
  dependencies:
    - name: ci
      eventSourceName: ci
      eventName: argo-demo
  triggers:
    - template:
        name: ci
        argoWorkflow:
          operation: submit
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: argo-demo-
              spec:
                ttlStrategy:
                  secondsAfterCompletion: 3600   # Delete completed workflow pods after 1 hour
                  secondsAfterSuccess: 3600      # Delete successful workflow pods after 1 hour
                  # secondsAfterFailure: 86400     # Delete failed workflows after 1 day
                arguments:
                  parameters:
                    - name: app-repo-url
                      value: http://172.17.0.1/root/my-app.git          # app repo url to create webhook
                    - name: argo-config-repo-url
                      value: http://172.17.0.1/root/argo-config.git     # argo-config repo url to change image tag
                    - name: app-clone-dest
                      value: /tmp/app
                    - name: argo-config-clone-dest
                      value: /tmp/argo-config
                    - name: argo-config-branch
                      value: main
                    - name: nexus-registry
                      value: host.docker.internal:8085
                artifactRepositoryRef:
                  configMap: artifact-repository                 # artifact repository configmap - MinIO
                entrypoint: ci
                templates:
                - name: ci
                  dag:
                    tasks:
                    - name: clone-repo-task                     # clone repo task
                      template: clone-repo                      # clone repo template
                    - name: build-push-task                     # build and push task
                      template: build-and-push                  # build and push template
                      arguments:                                # push artifact to with the image tag
                        artifacts:
                          - name: app-repo                      # artifact name
                            from: "{{tasks.clone-repo-task.outputs.artifacts.app-repo}}"     # artifact path
                      dependencies: [clone-repo-task]           # dependency on clone repo task
                    - name: update-manifest-task                # update manifest task
                      template: update-manifest                 # update manifest template
                      arguments:                                # use artifacts from build-push task
                        artifacts:
                          - name: argo-config-repo              # artifact name
                            from: "{{tasks.clone-repo-task.outputs.artifacts.argo-config-repo}}"     # artifact path
                      dependencies: [clone-repo-task, build-push-task]   # dependency on clone repo task and build-push task
                - name: clone-repo                          # clone repo template
                  outputs:                                  # output artifacts
                    artifacts:
                    - name: app-repo                        # artifact name
                      path: "{{workflow.parameters.app-clone-dest}}"              # artifact path
                    - name: argo-config-repo                                      # artifact name
                      path: "{{workflow.parameters.argo-config-clone-dest}}"      # artifact path
                  script:
                    image: alpine/git
                    command: [sh]
                    env:
                      - name: APP_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: app-repo-credentials
                            key: token
                      - name: CONFIG_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: config-repo-credentials
                            key: token
                    source: |
                      git clone http://root:${APP_TOKEN}@172.17.0.1/root/my-app.git {{workflow.parameters.app-clone-dest}}
                      git clone http://root:${CONFIG_TOKEN}@172.17.0.1/root/argo-config.git {{workflow.parameters.argo-config-clone-dest}}
                - name: build-and-push                      # build and push template
                  inputs:                                   # input artifacts
                    artifacts:
                    - name: app-repo                                   # input artifact name
                      path: "{{workflow.parameters.app-clone-dest}}"   # input artifact path
                  volumes:
                    - name: docker-config-secret            # use volume to read docker secret 
                      secret:
                        secretName: docker-config-secret    # use docker config secret to access private image registry
                  container:
                    readinessProbe:
                      exec:
                        command: [ sh, -c, "buildctl debug workers" ]
                    image: moby/buildkit:v0.9.3-rootless
                    volumeMounts:
                      - name: docker-config-secret
                        mountPath: /.docker
                    workingDir: "{{workflow.parameters.app-clone-dest}}"
                    env:
                      - name: BUILDKITD_FLAGS
                        value: --oci-worker-no-process-sandbox
                      - name: DOCKER_CONFIG
                        value: /.docker
                    command:
                      - buildctl-daemonless.sh
                    args:
                      - build
                      - --frontend
                      - dockerfile.v0
                      - --local
                      - context=.
                      - --local
                      - dockerfile=.
                      - --output
                      - type=image,name={{workflow.parameters.nexus-registry}}/argo-demo/nginx:{{workflow.uid}},push=true,registry.insecure=true
                    securityContext:
                      privileged: true
                - name: update-manifest
                  inputs:
                    artifacts:
                      - name: argo-config-repo
                        path: "{{workflow.parameters.argo-config-clone-dest}}"
                  script:
                    image: alpine/git
                    workingDir: "{{workflow.parameters.argo-config-clone-dest}}/argo-rollouts"
                    command: [sh]
                    env:                                    # read credentials from secret
                      - name: GITLAB_USERNAME
                        valueFrom:
                          secretKeyRef:
                            name: config-repo-credentials
                            key: username
                      - name: GITLAB_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: config-repo-credentials
                            key: token
                    source: |
                      sed -i 's|image: host.docker.internal:8085/argo-demo/nginx:.*|image: {{workflow.parameters.nexus-registry}}/argo-demo/nginx:{{workflow.uid}}|' nginx-rollouts.yaml      
                      git config user.name "$GITLAB_USERNAME"
                      git config user.email "your-email@example.com"
                      git add .
                      git commit -m "image.tag has been changed to {{workflow.uid}}"

                      git remote set-url origin http://${GITLAB_USERNAME}:${GITLAB_TOKEN}@172.17.0.1/root/argo-config.git
                      git push origin {{workflow.parameters.argo-config-branch}}
-------------------------------------------------






ROLLOUT
-------

Every resource in this folder will be automatically created by ArgoCD when we deploy aour application !!!

argo-rollouts
   |-- nginx-rollouts.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: nginx-rollouts
spec:
  revisionHistoryLimit: 5               # Keep only last 5 revisions (default is 10)
  replicas: 5
  strategy:
    canary:
      steps:
      - setWeight: 20                   # first step is 20% of the traffic - 1 of 5 pods
      - pause: {}                       # manual promotion required
      - setWeight: 40                   # second step is 40% of the traffic - 2 of 5 pods
      - pause: {duration: 10s}          # auto promotion afer 10 seconds
      - setWeight: 60                   # third step is 60% of the traffic - 3 of 5 pods
      - pause: {duration: 20s}          # auto promotion afer 20 seconds
      - setWeight: 80                   # fourth step is 80% of the traffic - 4 of 5 pods
      - pause: {duration: 1m}           # auto promotion afer 1 minute
  selector:
    matchLabels:
      app: nginx-rollouts
  template:
    metadata:
      labels:
        app: nginx-rollouts
    spec:
      containers:
      - name: nginx-rollouts
        image: host.docker.internal:8085/argo-demo/nginx:alpine         # used image from Nexus repository
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 8080
    targetPort: 80
  selector:
    app: nginx-rollouts
-------------------------------------------------


argo-rollouts
   |-- nginx-ingress.yaml
-------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: argo-demo                    # Note: argo-demo namespace, not argo-events
spec:
  ingressClassName: nginx
  rules:
  - host: argo.demo                       # Access via http://argo.demo
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 8080
-------------------------------------------------


Add host address to Windows host list
	- Open PowerShell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 argo-demo'
		- save the file and exit

Add host address to Linux host list
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 argo-demo'
	- save changes and exit - escape, :wq!, enter

This is the ingress that expose the application.
After the rollout is created the app should be accessabel on http://argo.demo:32073/




argo-rollouts
   |-- webhook-ingress.yaml
-------------------------------------------------
# This ingress is matching the Docker internal network with exact path
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argo-events-ingress
  namespace: argo-events
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /push
        pathType: Exact
        backend:
          service:
            name: ci-eventsource-svc
            port:
              number: 12000
-------------------------------------------------

This ingress expose the webhook triggered from the push event in the GitLab repository and new workflow starts.
This ingress is not used if the GitLab server is not running on the same Docker network.




2. my-app - This is the app repository

my-app
  |-- Dockerfile
  |-- index.html


Dockerfile
-------------------------------------------------
FROM nginx:alpine
COPY index.html /usr/share/nginx/html/
-------------------------------------------------

index.html
-------------------------------------------------
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Welcome to My Website</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f4f4f4;
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        .container {
            text-align: center;
            color: #333;
        }

        h1 {
            font-size: 3em;
            margin-bottom: 0.5em;
            color: #3498db;
        }

        p {
            font-size: 1.5em;
            margin-top: 0;
            color: #777;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Welcome to My Website, Version 1.0</h1>
    </div>
</body>
</html>
-------------------------------------------------





DEPLOY THE SCENARIO
===================


DEPLOY THE APPLICATION 
----------------------

Deploy application in 'argocd' namespace (ArgoCD must visualize it)
	terminal --> k apply -f argo-app.yaml -n argocd

	# result: application.argoproj.io/argo-demo-application created

We can access ArgoCD - https://argocd.localhost:32074/
We can Access Argo Rollouts - http://rollouts.localhost:32073/rollouts/			# set argo-demo namespace
We can access Argo Workflows - https://argo-workflows.localhost:32074/workflows/	# set 'argo-events' namespace
We can access MinIO - http://minio.localhost:32073/browser
We can access Nexus - http://localhost:8081/#browse/browse
We can access GitLab - http://gitlab.localhost/

After the application is deployed we can access it on http://argo.demo:32073/


We can see the running pods in argo-events namespace
	terminal --> k get pods -n argo-events

	# result:
	NAME                                  READY   STATUS    RESTARTS   AGE
	controller-manager-59884fd695-5w557   1/1     Running   0          8h
	eventbus-default-stan-0               2/2     Running   0          31m
	eventbus-default-stan-1               2/2     Running   0          31m
	eventbus-default-stan-2               2/2     Running   0          31m
	events-webhook-588ccdfcb5-pl5kr       1/1     Running   0          8h


List the serrvices in argo-events namespace
	terminal --> k get svc -n argo-events

	# result:
	NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
	eventbus-default-stan-svc   ClusterIP   None            <none>        4222/TCP,6222/TCP,8222/TCP   32m
	events-webhook              ClusterIP   10.96.146.191   <none>        443/TCP                      8h



DEPLOY EVENT SOURCE
-------------------

Deploy the event source manifest
	terminal --> k apply -f event-source.yaml

	# result:
	eventsource.argoproj.io/ci created
	ingress.networking.k8s.io/argo-demo-ingress created


Deploy sensor in argo-events namespace
	terminal --> k apply -f sensor.yaml -n argo-events

	# result: sensor.argoproj.io/ci created



List pods in argo-events namespace again
	terminal --> k get pods -n argo-events

	# result:
	NAME                                    READY   STATUS    RESTARTS   AGE
	ci-eventsource-p7bcl-7bd77fc7bb-8hcc2   1/1     Running   0          38s	# this pod is event source
	controller-manager-59884fd695-5w557     1/1     Running   0          8h
	eventbus-default-stan-0                 2/2     Running   0          35m
	eventbus-default-stan-1                 2/2     Running   0          35m
	eventbus-default-stan-2                 2/2     Running   0          35m
	events-webhook-588ccdfcb5-pl5kr         1/1     Running   0          8h


List the serrvices in argo-events namespace again
	terminal --> k get svc -n argo-events

	# result:
	NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
	ci-eventsource-svc          ClusterIP   10.96.206.159   <none>        12000/TCP                    2m9s	 # created svc
	eventbus-default-stan-svc   ClusterIP   None            <none>        4222/TCP,6222/TCP,8222/TCP   32m
	events-webhook              ClusterIP   10.96.146.191   <none>        443/TCP                      8h


Check the webhook in gitlab repo - http://gitlab.localhost/root/my-app/-/hooks
	- edit the webhook and set Trigger: Push events: All branches



MAKE A CHANGE
-------------

When we create and push change to our app

index.html
-------------------------------------------------
    <div class="container">
        <h1>Welcome to My Website, Version 2.0</h1>
    </div>
-------------------------------------------------
# from '1.0' to '2.0' - changed subversion

Commit changes
	terminal --> git add .
	terminal --> git commit -m "version 2"
 	terminal --> git push


The change in the repostiory will trigger the webhook and new workflow will start.

After the workflow finishes the used image will be changed in the rollout.
The application will autoamtic sync after 3 minutes.
The application will start update (canary deployment will start).

We can fooolow the rollout and promote the first step on http://rollouts.localhost:32073/rollouts/ or with Argo UI in the argo-demo namepsace.
	terminal --> kargo promote nginx-rollouts -n argo-demo

	# result: rollout 'nginx-rollouts' promoted

After the rollout is finished and the application is running with the new version we can test it on http://argo.demo:32073/
We should see message 'Welcome to My Website, Version 2.0'


If we delete the app and we redeploy it we need to recreate the webhook also because of the flag 'deleteHookOnFinish: true' in the event-source.yaml manifest.
	- Find ci deployment
		terminal --> k get deploy -n argo-events

		# result:
		NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
		ci-eventsource-p7bcl   1/1     1            1           21h	# target deployment
		ci-sensor-24b44        1/1     1            1           8h
		controller-manager     1/1     1            1           30h
		events-webhook         1/1     1            1           30h

	- Restart the deployment	
		terminal --> kubectl rollout restart deploy/ci-eventsource-p7bcl -n argo-events

		# result: deployment.apps/ci-eventsource-p7bcl restarted

	Now the webhook is recreated and we can trigger workflows again with push events.
		- Edit the webhook and set 'Push events: All branches' !!!
		- Save changes

	We can now make changes to the app and the workflow will be triggered
		




18. Add inline analysis
=======================

We will use simple flask analysis application to test the main application accessability.

We will create analysis application image nad deploy it in separate namespace so we dont mix applications deployments.


Create canary-analysis namespace
	terminal --> k create ns canary-analysis

	# result: namespace/canary-analysis created


Analysis Application resources
	- analysis-app.py
	- Dockerfile

analysis-app.py
-------------------------------------------------
### WE are using a flask script to get the url of the application and try to connect to it 5 times at 5 seconds intervals and returns one of the true or false values based on "200 response_code" in a json format, if the success_rate is 80% or more , it returns "true" value, Otherwise it returns "false" value
from flask import Flask, request, jsonify
import requests, time

app = Flask(__name__)

@app.route('/measure_success_rate', methods=['POST'])
def measure_success_rate():
    url = request.json.get('url')

    if not url:
        return jsonify({'error': 'URL is missing in the request body'}), 400

    success_count = 0

    for _ in range(5):
        try:
            response = requests.get(url)
        except requests.exceptions.RequestException:
            pass
        else:
            if response.status_code == 200:
                success_count += 1
        time.sleep(5)
        
    success_rate = (success_count / 5) * 100
    if int(success_rate) >= 80:
        return jsonify({'data': {'ok': 'true'}})
    else:
        return jsonify({'data': {'ok': 'false'}})

if __name__ == '__main__':
    # Run on all interfaces so it's accessible within the cluster
    app.run(host='0.0.0.0', port=5001)
-------------------------------------------------


Dockrfile
-------------------------------------------------
FROM python:3.13-slim

WORKDIR /app

# Install required Python packages
RUN pip install flask requests

# Copy the Flask app
COPY analysis-app.py .

# Expose the port
EXPOSE 5001

# Run the Flask app
CMD ["python", "analysis-app.py"]
-------------------------------------------------


Build image for web-analysis
	trminal --> docker build -t web-analysis .

Load the image into the cluster
	terminal --> kind load docker-image web-analysis --name kind




We will deploy cluster analysis template canary-analysis.yaml so it can be used across different namespaces.

canary-analysis.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: ClusterAnalysisTemplate       # Set the template as a ClusterAnalysisTemplate, no namespace configured
metadata:
  name: success-rate
spec:
  args:
  - name: service_port      # This is the variable related to the ip address where the analysis-app.py is located
    value: "5001"           # port of the analysis app
    # WE are using a flask script to get the url of the application and try to connect to it 5 times at 5 seconds intervals and returns one of the true or 
    # false values based on "200 response_code" in a json format, if the success_rate is 80% or more , it returns "true" value, Otherwise it returns "false" value,
    # you can find this simple app in the current directory (its name is analysis-app.py).
  metrics:
  - name: success-rate
    successCondition: result == "true"
    provider:
      web:
        method: POST
        # Use Kubernetes service DNS name instead of external IP
        url: "http://web-analysis.canary-analysis.svc.cluster.local:{{args.service_port}}/measure_success_rate"
        timeoutSeconds: 50
        headers:
          - key: Content-Type             # if body is a json, it is recommended to set the Content-Type
            value: "application/json"
        jsonBody:                         # If using jsonBody Content-Type header will be automatically set to json
          url: "http://nginx-service.argo-demo.svc.cluster.local:8080"   # this is the url of the application svc with local docker address
        jsonPath: "{$.data.ok}"
-------------------------------------------------

Deploy the canary analysy
	terminal --> k apply -f canary-analysis.yaml

	# result: clusteranalysistemplate.argoproj.io/success-rate created


Create deployment for the analysis app

web-analysis-deployment.yaml
-----------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-analysis
  namespace: canary-analysis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web-analysis
  template:
    metadata:
      labels:
        app: web-analysis
    spec:
      containers:
      - name: web-analysis
        image: web-analysis
        imagePullPolicy: Never
        ports:
        - containerPort: 5001
---
apiVersion: v1
kind: Service
metadata:
  name: web-analysis
  namespace: canary-analysis
spec:
  selector:
    app: web-analysis
  ports:
  - protocol: TCP
    port: 5001
    targetPort: 5001
  type: ClusterIP
-----------------------------------------------

Deploy the analysis application
	terminal --> k apply -f web-analysis-deployment.yaml -n canary-analysis

	# result:
	deployment.apps/web-analysis created
	service/web-analysis created

Check if the serice is running
	terminal --> kubectl get pods -n canary-analysis -l app=web-analysis

	# result:
	NAME                            READY   STATUS    RESTARTS   AGE
	web-analysis-5f9bc64f67-2tgn6   1/1     Running   0          6m29s

We can check the logs of the analysis app
	terminal --> kubectl logs -f deployment/web-analysis -n canary-analysis



Add the analysis to the rollout
-------------------------------

nginx-rollouts.yaml
-----------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: nginx-rollouts
  namespace: argo-demo
spec:
  revisionHistoryLimit: 5               # Keep only last 5 revisions (default is 10)
  replicas: 5
  strategy:
    canary:
      steps:
      - setWeight: 20                   # first step is 20% of the traffic - 1 of 5 pods
      - pause: {}                       # manual promotion required
      - analysis:                           # This is an Inline Analysis
          templates:
          - templateName: success-rate
            clusterScope: true          # using a cluster-scoped AnalysisTemplate (deployed in another namespace)
          args:
          - name: service_port              # This is the variable related to the port of analysis-app.py flask app
            value: "5001"
      - setWeight: 40                   # second step is 40% of the traffic - 2 of 5 pods
      - pause: {duration: 10s}          # auto promotion afer 10 seconds
      - setWeight: 60                   # third step is 60% of the traffic - 3 of 5 pods
      - pause: {duration: 20s}          # auto promotion afer 20 seconds
      - setWeight: 80                   # fourth step is 80% of the traffic - 4 of 5 pods
      - pause: {duration: 1m}           # auto promotion afer 1 minute
  selector:
    matchLabels:
      app: nginx-rollouts
  template:
    metadata:
      labels:
        app: nginx-rollouts
    spec:
      containers:
      - name: nginx-rollouts
        image: host.docker.internal:8085/argo-demo/nginx:529057ec-53cb-4100-9390-87a8f64dac30
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 8080
    targetPort: 80
  selector:
    app: nginx-rollouts
-----------------------------------------------


Push the changes to the argo-config repository
	terminal --> git add .
	terminal --> git commit -m "added canary analysis to rollout"
	terminal --> git push

The application will detect change in the argo-config repository and will sync the chnages.
Now we should have additional step in the visualized roolout in Argo Rollouts UI.
We can check the analysis step in Argo Rollouts UI on http://rollouts.localhost:32073/rollouts/


We can make a change to the application and test the analysis

MAKE A CHANGE
-------------

When we create and push change to our app

index.html
-------------------------------------------------
    <div class="container">
        <h1>Welcome to My Website, Version 3.0</h1>
    </div>
-------------------------------------------------
# from '1.0' to '2.0' - changed subversion

Commit changes
	terminal --> git add .
	terminal --> git commit -m "version 3"
 	terminal --> git push


The change in the repostiory will trigger the webhook and new workflow will start.

After the workflow finishes the used image will be changed in the rollout.
The application will autoamtic sync after 3 minutes.
The application will start update (canary deployment will start).

We can fooolow the rollout and promote the first step on http://rollouts.localhost:32073/rollouts/ or with Argo UI in the argo-demo namepsace.
	terminal --> kargo promote nginx-rollouts -n argo-demo

	# result: rollout 'nginx-rollouts' promoted

After the promotion the analysis step will start. If it is successfull the rollout will continue with the next step of the deployment. If fails the deployment will be cancelled and the old version of the application will be rolled back.





19. Backup Docker Desktop Kind clustr
=====================================

Export cluster configuration
----------------------------

List cluster
	terminal --> kind get clusters

	# result: kind

Export the cluster config
	terminal --> kind export kubeconfig --name kind

	# the config is save at C:\Users\your-user\.kube\config



Export cluster resources
------------------------

Export all resources (deployments, services, configs, and secrets)
	terminal -> kubectl get all --all-namespaces -o yaml > cluster-resources.yaml

Export also PersistentVolumes (if any)
	terminal --> kubectl get pv,pvc -o yaml >> cluster-resources.yaml



Save Docker images
------------------

Save Docker images used on the cluster in text file
	terminal --> kubectl get pods --all-namespaces -o jsonpath='{.items[*].spec.containers[*].image}' | ForEach-Object { $_ -replace ',','`n' } | Sort-Object -Unique | Out-File cluster-images.txt

We can see the used images and save important ones
	terminal --> docker save web-analysis -o web-analysis.tar


22. Restore Process
-------------------

Recreate cluster 
	terminal --> kind create cluster --config your-config.yaml

load images
	terminal --> kind load image-archive web-analysis.tar

import kubeconfig, and apply
	terminal --> kubectl apply -f cluster-resources.yaml











Example CI/CD pipeline configuration with GitHub
================================================


20. Install Nexus private image registry
========================================

1. Download and install Java from: https://www.oracle.com/java/technologies/downloads/#jdk25-windows

	Confirm java installation
		terminal --> java -version

2 Download Nexus Repository OSS (Open Source) for Windows - https://www.sonatype.com/products/nexus-community-edition-download

3. Install Nexus
	- unarchive on the PC and navigate to the folder
	- Open shell as administrator and run
		terminal --> .\install-nexus-service.bat

4. Start Nexus service
	terminal --> net start SonatypeNexusRepository

	Check if port 8081 is listening
		terminal --> netstat -ano | findstr :8081

5. Login to Nexus on http://localhost:8081/#login

	Find the initial generated credentials in the installed directory 'sonatype-work\nexus3\admin.password' or with shell
		terminal --> type E:\Installed\nexus-3.88.0-08-win-x86_64\sonatype-work\nexus3\admin.password

	Login to the app and finish the installtion. Relogin.
		Username: admin
		Password: admin123		(default)


Create Nexus repository - example usage
---------------------------------------
- Open Nexus - http://localhost:8081
- Go to Settings/Repositories/Create Repository/docker(hosted)
	- Name: argo-demo
	- Other Connectors
		- HTTP: 8085
	- Docker Registry API Support
		Select Checkbox "Allow clients to use the V1 API to interact with this repository"
	- Create Repository

- Go to Settings/Security/Realms
	- Set Docker Bearer Token Realm to Active
	- Save

 

Configure Docker Desktop to communicate with the created repository
	- Open Docker/Settings/Docker Engine
	- Add
	-------------------------------------------------
	{
	  "insecure-registries": ["localhost:8085"]
	}
	-------------------------------------------------
	- Apply and Restart



Test Docker connection with Nexus by pushing the image created earlier
	Login to Nesus true the configured address
		terminal --> docker login host.docker.internal:8085
		terminal --> admin
		terminal --> nexsus_password

	Retag (rename) and push the image we created earlier to the Nexus repository
		terminal --> docker tag nginx:v1 host.docker.internal:8085/argo-demo/nginx:alpine
		terminal --> docker push host.docker.internal:8085/argo-demo/nginx:alpine

	The image should be visualized in the Nexus repository
		- Go to http://localhost:8081/#browse/browse
		- Then navigate to v2/argo-demo/nginx/tags




Configure Argo Workflows to pull/push images from/to Nexus
----------------------------------------------------------

We need to allow Argo Workflows communication in the Docker Engine Settings to be able to access Nexus platofrm. In this case we are using Docker Desktop and we add the internal Docker host address and the port of the Nexus repository.

Configure Argo Workflows to communicate with Nexus repository
	- Open Docker/Settings/Docker Engine
	- Add
	-------------------------------------------------
	{
	  "insecure-registries": ["localhost:8085", "host.docker.internal:8085"]
	}
	-------------------------------------------------
	- Apply and Restart



Configure Argo Workflows to pull images from Nexus
--------------------------------------------------


Create containerd daemonset configuration

containerd-config-daemonset.yaml
-------------------------------------------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: containerd-registry-config
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: containerd-registry-config
  template:
    metadata:
      labels:
        name: containerd-registry-config
    spec:
      hostPID: true
      hostNetwork: true
      initContainers:
      - name: configure-containerd
        image: alpine:latest
        command:
        - sh
        - -c
        - |
          set -e
          
          # Check if configuration already exists
          if grep -q "host.docker.internal:8085" /host/etc/containerd/config.toml 2>/dev/null; then
            echo "Registry configuration already exists, skipping..."
            exit 0
          fi
          
          # Append registry configuration
          cat >> /host/etc/containerd/config.toml << 'EOF'

          [plugins."io.containerd.grpc.v1.cri".registry.mirrors."host.docker.internal:8085"]
            endpoint = ["http://host.docker.internal:8085"]
          [plugins."io.containerd.grpc.v1.cri".registry.configs."host.docker.internal:8085".tls]
            insecure_skip_verify = true
          EOF
          
          echo "Configuration added successfully"
          
          # Find and restart containerd process
          CONTAINERD_PID=$(nsenter -t 1 -m -u -i -n -p pgrep containerd | head -n 1)
          if [ -n "$CONTAINERD_PID" ]; then
            echo "Sending SIGHUP to containerd (PID: $CONTAINERD_PID)"
            nsenter -t 1 -m -u -i -n -p kill -HUP $CONTAINERD_PID
            sleep 2
            echo "Containerd reloaded"
          else
            echo "Warning: Could not find containerd process"
          fi
        securityContext:
          privileged: true
        volumeMounts:
        - name: containerd-config
          mountPath: /host/etc/containerd
      containers:
      - name: pause
        image: registry.k8s.io/pause:3.9
        resources:
          requests:
            cpu: 1m
            memory: 4Mi
      volumes:
      - name: containerd-config
        hostPath:
          path: /etc/containerd
          type: Directory
      tolerations:
      - operator: Exists
-------------------------------------------------

Apply the deamonset
	terminal --> kubectl apply -f containerd-config-daemonset.yaml

	# result: daemonset.apps/containerd-registry-config created

Test nodes communication
	terminal --> k get nodes

Wait 2 minutes and delete the daemonset
	terminal --> kubectl delete -f containerd-config-daemonset.yaml



Give Argo Workflows access to Nexus
-----------------------------------

For this task we need to create Docker secret called 'docker-config-secret' in our Argo Workflows working 'argo-events' namespace. We have to mount our docker credentials to this sescret.

We need to create docker-config.json file and set the Nexus credentials so the workflow can pull and push images.

1. Encode the nexus creadentials with shell
	terminal --> $auth = [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes("admin:admin123"))
$auth
	result: YWRtaW46YWRtaW4xMjM=

2. Create docker-config.json
-----------------------------------------
{
  "auths": {
    "host.docker.internal:8085": {
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
-----------------------------------------

From the folder location create Docker secret
	terminal --> kubectl create secret generic docker-config-secret --from-file=config.json=./docker-config.json -n argo-events

	# result: secret/docker-config-secret created




21. Install MinIO artifacts storage with Helm
=============================================

MinIO shared storage service that can be used by workflows in any namespace. It is commonly used module that is installed in 'argo' namespace as Argo Workflows and should be separated from the workload namespaces in our cluster.

Prerequisites
	- kubectl CLI tool
	- Helm CLI tool

MinIO shared storage service that can be used by workflows in any namespace. It is commonly used module that is installed in 'argo' namespace as Argo Workflows and should be separated from the workload namespaces in our cluster.

INSTALL MINIO WIHT HELM
-----------------------
1. Install MinIO helm repository on our PC
	terminal --> helm repo add minio https://charts.min.io/
	terminal --> helm repo update


2. Install MinIO chart
	terminal --> helm install argo-artifacts minio/minio --set resources.requests.memory=512Mi --set replicas=1 --set persistence.enabled=false --set mode=standalone --set rootUser=admin --set rootPassword=password123 --set buckets[0].name=my-bucket --set buckets[0].policy=none --set buckets[0].purge=false -n argo


3. Create credentials secret in working namespace
	terminal --> k create secret generic my-minio-cred --from-literal=access-key=admin --from-literal=secret-key=password123 -n argo-events

	# result: secret/my-minio-cred created


4. Set Ingress controller for MinIO to be externally accessed 

minio-ingress.yaml
-------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minio-console-ingress
  namespace: argo
spec:
  ingressClassName: nginx
  rules:
  - host: minio.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: argo-artifacts-minio-console
            port:
              number: 9001
-------------------------------------------------

Apply the ingress manifest
	terminal --> k apply -f minio-ingress.yaml -n argo

	# result: ingress.networking.k8s.io/minio-console-ingress created

	
Add host address to Windows host list on Windows
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 minio.localhost'
		- save the file and exit

Add host address to Windows host list on Linux
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 minio.localhost'
	- save changes and exit - escape, :wq!, enter


ACCESS MINIO
------------

Decode the username (rootUser) with shell
	terminal --> kubectl get secret argo-artifacts-minio -n argo -o jsonpath='{.data.rootUser}' | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([Convert]::FromBase64String($_.Trim())) }

	# result: admin

Decode the password (rootPassword) with shell
	terminal --> kubectl get secret argo-artifacts-minio -n argo -o jsonpath='{.data.rootPassword}' | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([Convert]::FromBase64String($_.Trim())) }

	# result: password123


Access the MinIO app on http://minio.localhost:32073/login
	- admin
	- password123

We can create bucket if we haven't with the installation command.
Create a bucket named 'my-bucket'. We will store our artifacts in this bucket.


USE MINIO
---------
To use artifacts we need to configure MinIO repository to store the artifacts from the working namespace. MinIO repository is configured in minio-artifact-repo-cm.yaml configmap file below.

minio-artifact-repo-cm.yam
-------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  # If you want to use this config map by default, name it "artifact-repositories". Otherwise, you can provide a reference to a
  # different config map in `artifactRepositoryRef.configMap` in the workflow.
  name: artifact-repository           # this name is not the default name
  annotations:
    # v3.0 and after - if you want to use a specific key, put that key into this annotation.
    workflows.argoproj.io/default-artifact-repository: minio-artifact-repo
data:
  minio-artifact-repo: |
    s3:
      bucket: my-bucket
      endpoint: argo-artifacts-minio.argo:9000
      insecure: true
      accessKeySecret:
        name: my-minio-cred
        key: access-key 
      secretKeySecret:
        name: my-minio-cred
        key: secret-key
-------------------------------------------------

Create the configmap into the working namespace
	terminal --> kubectl apply -f minio-artifact-repo-cm.yaml -n argo-events

	# result: configmap/artifact-repository created

Confirm ConfigMap creation
	terminal --> kubectl get configmap artifact-repository -n <namespace>

	# result:
	NAME                  DATA   AGE
	artifact-repository   1      104m





22. INSTALL NGROK
=================
We will expose our PC with public domain with ngrok. Download ngrok - https://dashboard.ngrok.com/get-started

Download ngrok for Windows 64bit and extract it in C:\Users\your_user\ngrok.exe

Set Path to the Environment Variables - open power shell as administrator
	admin terminal --> $oldPath = [Environment]::GetEnvironmentVariable("Path", "User")
	admin terminal --> [Environment]::SetEnvironmentVariable("Path", "$oldPath;C:\Users\your-user", "User")   # change user

Test ngrok installation
	terminal --> ngrok version
	terminal --> Get-Command ngrok

	# result: ngrok version 3.35.0
	# result: Application     ngrok  3.35.0     C:\Users\your-user\ngrok.exe
	

Run the following command to add your authtoken to the default ngrok.yml configuration file.
	terminal --> ngrok config add-authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

We will use this command later to expose our PC with the command
	terminal --> ngrok http 32073					# USE LATER !!!

Copy the address shown in the next section to set it in the Argo event-source.yaml manifest.

To prevent usage of the domain in by external requests we will set additional webhook secret.

	





23. ArgoCD Setup with GitHub
============================

Create 2 GitHub repositories
	1. my-app
	2. argo-config


MY-APP REPO
-----------

This repository is contains simple application that we will use to test this example ArgoCD setup. When application is changed it will activate GitHub webhook that will trigger ArgoCD workflow.
	
my-app
  |-- Dockerfile
  |-- index.html

Dockerfile
-------------------------------------------------
FROM nginx:alpine
COPY index.html /usr/share/nginx/html/
-------------------------------------------------

index.html
-------------------------------------------------
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Welcome to My Website</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            background-color: #f4f4f4;
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
        }

        .container {
            text-align: center;
            color: #333;
        }

        h1 {
            font-size: 3em;
            margin-bottom: 0.5em;
            color: #3498db;
        }

        p {
            font-size: 1.5em;
            margin-top: 0;
            color: #777;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Welcome to My Website, Version 1.0</h1>
    </div>
</body>
</html>
-------------------------------------------------





ARGO-CONFIG REPO
----------------

This repository contains ArgoCD configuration manifests files. It will trigger application update after the application image tag is changed by the workflow.

argo-config
	|--argo-cd
	|  |-- argo-app.yaml
	|
	|--argo-events
	|  |-- event-source.yaml
	|  |-- sensor.yaml
        |
	|--canary-analysis
	|  |--analysis-app
 	|  |   |-- analysis-app.py
	|  |   |-- Dockerfile
 	|  |
	|  |-- camary-analysis.yaml
	|  |-- web-analysis-depoyment.yaml
	|
	|--argo-rollouts
	   |-- nginx-ingress.yaml
	   |-- nginx-rollouts.yaml
	   |-- webhook-ingress.yaml


We will go true all used resources and after we will deploy them.

ARGO-CD
-------

|--argo-cd
    |-- argo-app.yaml

In the argo-app.yaml CRD we set the argo-config repository path and branch, CRD will be deployed in argocd namespace adn the application will be deployed in argo-demo namespace. Automated synchronization , prune and selfheal are enabled.

argo-cd/argo-app.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argo-demo-application                 # application name
  namespace: argocd                           # application namespace - visualize with ArgoCD           
spec:
  destination:
    namespace: argo-demo                      # destination namespace - deployed in the argo-demo namespace
    server: https://kubernetes.default.svc    # destination server
  project: default                            # project
  source:
    path: argo-rollouts                       # path to the rollout manifest
    repoURL: https://github.com/entermix123/argo-config.git     # config repostiory address
    targetRevision: main                                        # target revision - can be branch, tag or commit
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:                                   # sync options section
      prune: true                                # prune resources - delete resources that are not in the manifest
      selfHeal: true                             # self heal enabled - recreate deleted resources
-------------------------------------------------




ARGO EVENTS
-----------

|--argo-events
   |-- event-source.yaml
   |-- sensor.yaml

In event-source CRD we set our public domain (GitHub must access our argo configuration and trigger the workflow by the application webhook), usage of secret for our GitHub credentials and ingress for the webhook. We have to generate secret and set it in the event source manifest to prevent external requests triggering our workflow and waste our resources. 

In this example we use secret for GtHub webhook because we use ngrok domain exposure. In production we use different setup and secret method can be changed with other methods.

argo-events/event-source.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: EventSource				# event source obejct
metadata:
  name: ci
  namespace: argo-events
spec:
  service:
    ports:
      - port: 12000
        targetPort: 12000
  github:                           # Changed from gitlab to github
    argo-demo:
      owner: entermix123            # Your GitHub username/org
      repository: my-app            # Repository name
      webhook:
        endpoint: /push
        port: "12000"
        url: https://xxxxxxxxxxx.ngrok-free.app                    # Must be publicly accessible! Change when ngrok restarts
        # GitHub webhooks MUST reach a public endpoint
        # Options:
        # 1. Use ngrok: http://your-ngrok-url.ngrok.io
        # 2. Use your public IP: http://YOUR_PUBLIC_IP:32073
        # 3. Use a domain: http://webhook.yourdomain.com
      events:
        - push                    # PushEvent (GitHub)
      apiToken:                   # GitHub apiToken
        key: token
        name: app-repo-credentials
      webhookSecret:                  # GitHub webhookSecret - prevent requests outside GitHub
        name: github-webhook-secret   # GitHub webhookSecret name
        key: webhook-secret           # GitHub webhookSecret key
      insecure: true
      active: true
      contentType: json
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argo-events-ingress       # ingress for the webhook access
  namespace: argo-events
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /push
        pathType: Prefix
        backend:
          service:
            name: ci-eventsource-svc        # the backend service name created by the event source
            port:
              number: 12000

# Use if the ingress is connecting out of Docker network
# apiVersion: networking.k8s.io/v1
# kind: Ingress                                  # ingress
# metadata:
#   name: argo-demo-ingress
#   namespace: argo-events
# spec:
#   ingressClassName: nginx
#   rules:
#   - host: argo.events                         # host name of the ingress 
#     http:
#       paths:
#       - path: /
#         pathType: Prefix
#         backend:
#           service:
#             name: ci-eventsource-svc          # the backend service name created by the event source
#             port:
#               number: 12000
-------------------------------------------------



In sensor.yaml we have configured all tasks, their templates and commands for execution.

argo-events/sensor.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: Sensor                  # CRD type 
metadata:
  name: ci                    # sensor name
  namespace: argo-events      # sensor namespace
spec:
  template:
    serviceAccountName: operate-workflow-sa            # service account name
  dependencies:
    - name: ci                                         # dependency name
      eventSourceName: ci                              # event source name
      eventName: argo-demo                             # destination namespace
  triggers:                                            # triggers
    - template:
        name: ci                                       # trigger template name                 
        argoWorkflow:
          operation: submit                            # operation
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1         # resource api version
              kind: Workflow                           # resource kind
              metadata:
                generateName: argo-demo-               # workflow name with suffix
              spec:
                ttlStrategy:
                  secondsAfterCompletion: 3600         # workflow will be deleted after 1 hour if completed
                  secondsAfterSuccess: 3600            # workflow will be deleted after 1 hour if succeeded
                  # secondsAfterFailure: 3600          # workflow will be deleted after 1 hour if failed - disabled fot investigation
                arguments:
                  parameters:
                    - name: app-repo-url
                      value: https://github.com/entermix123/my-app.git            # app repo for cerating webhook
                    - name: argo-config-repo-url
                      value: https://github.com/entermix123/argo-config.git       # argo config repo for changing the image tag
                    - name: app-clone-dest
                      value: /tmp/app
                    - name: argo-config-clone-dest
                      value: /tmp/argo-config
                    - name: argo-config-branch
                      value: main
                    - name: nexus-registry
                      value: host.docker.internal:8085              # nexus registry url
                artifactRepositoryRef:
                  configMap: artifact-repository                    # artifact repository   
                entrypoint: ci
                templates:
                - name: ci
                  dag:                                  # using dag template evoker
                    tasks:
                    - name: clone-repo-task             # task 1 name - clone the app repo
                      template: clone-repo              # task 1 template
                    - name: build-push-task             # task 2 name - build and push the image
                      template: build-and-push          # task 2 template
                      arguments:
                        artifacts:                      # use artifact 1 from task 1
                          - name: app-repo              # artifacts name
                            from: "{{tasks.clone-repo-task.outputs.artifacts.app-repo}}"      # artifact 1 from task 1 path
                      dependencies: [clone-repo-task]                                         # task 2 depends on task 1
                    - name: update-manifest-task        # task 3 name - update the manifest
                      template: update-manifest         # task 3 template
                      arguments:
                        artifacts:                      # use artifact 2 from task 1
                          - name: argo-config-repo      # artifacts name
                            from: "{{tasks.clone-repo-task.outputs.artifacts.argo-config-repo}}"    # artifact 2 from task 1 path
                      dependencies: [clone-repo-task, build-push-task]                              # task 3 depends on task 1 and task 2
                - name: clone-repo              # task 1 template
                  outputs:
                    artifacts:                  # output artifacts
                    - name: app-repo            # artifacts name
                      path: "{{workflow.parameters.app-clone-dest}}"            # artifact 1 path
                    - name: argo-config-repo                                    # artifacts name
                      path: "{{workflow.parameters.argo-config-clone-dest}}"    # artifact 2 path
                  script:
                    image: alpine/git                             # script image
                    command: [sh]                                 # script command
                    env:
                      - name: APP_TOKEN                           # name of the environment variable 
                        valueFrom:
                          secretKeyRef:                           # secret key reference
                            name: app-repo-credentials            # secret name
                            key: token                            # secret key
                      - name: CONFIG_TOKEN                        # name of the environment variable
                        valueFrom:
                          secretKeyRef:                           # secret key reference
                            name: config-repo-credentials         # secret name
                            key: token                            # secret key
                    source: |
                      # Clone using GitHub token authentication
                      git clone https://entermix123:${APP_TOKEN}@github.com/entermix123/my-app.git {{workflow.parameters.app-clone-dest}}
                      git clone https://entermix123:${CONFIG_TOKEN}@github.com/entermix123/argo-config.git {{workflow.parameters.argo-config-clone-dest}}
                - name: build-and-push                                  # task 2 template
                  inputs:
                    artifacts:                                          # input artifacts
                    - name: app-repo                                    # artifacts name
                      path: "{{workflow.parameters.app-clone-dest}}"    # artifact 1 path - app image
                  volumes:
                    - name: docker-config-secret                        # use secret for docker config to access Neus registry
                      secret:
                        secretName: docker-config-secret                # secret name
                  container:
                    readinessProbe:                                     # container readiness probe for building the image
                      exec:
                        command: [ sh, -c, "buildctl debug workers" ]
                    image: moby/buildkit:v0.9.3-rootless
                    volumeMounts:
                      - name: docker-config-secret
                        mountPath: /.docker
                    workingDir: "{{workflow.parameters.app-clone-dest}}"
                    env:
                      - name: BUILDKITD_FLAGS
                        value: --oci-worker-no-process-sandbox
                      - name: DOCKER_CONFIG
                        value: /.docker
                    command:
                      - buildctl-daemonless.sh
                    args:
                      - build
                      - --frontend
                      - dockerfile.v0
                      - --local
                      - context=.
                      - --local
                      - dockerfile=.
                      - --output
                      - type=image,name={{workflow.parameters.nexus-registry}}/argo-demo/nginx:{{workflow.uid}},push=true,registry.insecure=true
                    securityContext:
                      privileged: true
                - name: update-manifest                                             # task 3 template
                  inputs:
                    artifacts:                                                      # input artifacts
                      - name: argo-config-repo                                      # artifact 2 from task 1 - changed image tag
                        path: "{{workflow.parameters.argo-config-clone-dest}}"      # artifact 2 path
                  script:
                    image: alpine/git                                               # script image
                    workingDir: "{{workflow.parameters.argo-config-clone-dest}}/argo-rollouts"      # script working directory
                    command: [sh]
                    env:                              # use environment variables
                      - name: GITHUB_USERNAME         # name of the environment variable
                        valueFrom:
                          secretKeyRef:                       # secret key reference
                            name: config-repo-credentials     # secret name
                            key: username                     # secret key
                      - name: GITHUB_TOKEN                    # name of the environment variable
                        valueFrom:
                          secretKeyRef:
                            name: config-repo-credentials     # secret name
                            key: token                        # secret key
                    source: |
                      sed -i 's|image: host.docker.internal:8085/argo-demo/nginx:.*|image: {{workflow.parameters.nexus-registry}}/argo-demo/nginx:{{workflow.uid}}|' nginx-rollouts.yaml
                      git config user.name "$GITHUB_USERNAME"
                      git config user.email "your-email@example.com"
                      git add .
                      git commit -m "Update image tag to {{workflow.uid}}"
                      
                      # Push using GitHub token authentication
                      git remote set-url origin https://${GITHUB_USERNAME}:${GITHUB_TOKEN}@github.com/entermix123/argo-config.git
                      git push origin {{workflow.parameters.argo-config-branch}}
-------------------------------------------------





CANARY ANALYSIS
---------------

|--canary-analysis
   |--analysis-app
   |   |-- analysis-app.py
   |   |-- Dockerfile
   |
   |-- camary-analysis.yaml
   |-- web-analysis-depoyment.yaml



We will use simple flask analysis application to test the main application accessability.

We will create analysis application image and deploy it in separate namespace so we don't mix applications deployments.


Analysis Application resources
	- analysis-app.py
	- Dockerfile

analysis-app.py
-------------------------------------------------
### WE are using a flask script to get the url of the application and try to connect to it 5 times at 5 seconds intervals and returns one of the true or false values based on "200 response_code" in a json format, if the success_rate is 80% or more , it returns "true" value, Otherwise it returns "false" value
from flask import Flask, request, jsonify
import requests, time

app = Flask(__name__)

@app.route('/measure_success_rate', methods=['POST'])
def measure_success_rate():
    url = request.json.get('url')

    if not url:
        return jsonify({'error': 'URL is missing in the request body'}), 400

    success_count = 0

    for _ in range(5):
        try:
            response = requests.get(url)
        except requests.exceptions.RequestException:
            pass
        else:
            if response.status_code == 200:
                success_count += 1
        time.sleep(5)
        
    success_rate = (success_count / 5) * 100
    if int(success_rate) >= 80:
        return jsonify({'data': {'ok': 'true'}})
    else:
        return jsonify({'data': {'ok': 'false'}})

if __name__ == '__main__':
    # Run on all interfaces so it's accessible within the cluster
    app.run(host='0.0.0.0', port=5001)
-------------------------------------------------


Dockrfile
-------------------------------------------------
FROM python:3.13-slim

WORKDIR /app

# Install required Python packages
RUN pip install flask requests

# Copy the Flask app
COPY analysis-app.py .

# Expose the port
EXPOSE 5001

# Run the Flask app
CMD ["python", "analysis-app.py"]
-------------------------------------------------


We will deploy cluster analysis template canary-analysis.yaml so it can be used across different namespaces.

canary-analysis.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: ClusterAnalysisTemplate       # Set the template as a ClusterAnalysisTemplate, no namespace configured
metadata:
  name: success-rate
spec:
  args:
  - name: service_port      # This is the variable related to the ip address where the analysis-app.py is located
    value: "5001"           # port of the analysis app
    # WE are using a flask script to get the url of the application and try to connect to it 5 times at 5 seconds intervals and returns one of the true or 
    # false values based on "200 response_code" in a json format, if the success_rate is 80% or more , it returns "true" value, Otherwise it returns "false" value,
    # you can find this simple app in the current directory (its name is analysis-app.py).
  metrics:
  - name: success-rate
    successCondition: result == "true"
    provider:
      web:
        method: POST
        # Use Kubernetes service DNS name instead of external IP
        url: "http://web-analysis.canary-analysis.svc.cluster.local:{{args.service_port}}/measure_success_rate"
        timeoutSeconds: 50
        headers:
          - key: Content-Type             # if body is a json, it is recommended to set the Content-Type
            value: "application/json"
        jsonBody:                         # If using jsonBody Content-Type header will be automatically set to json
          url: "http://nginx-service.argo-demo.svc.cluster.local:8080"   # this is the url of the application svc with local docker address
        jsonPath: "{$.data.ok}"
-------------------------------------------------



Create deployment for the analysis app.

web-analysis-deployment.yaml
-----------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-analysis
  namespace: canary-analysis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web-analysis
  template:
    metadata:
      labels:
        app: web-analysis
    spec:
      containers:
      - name: web-analysis
        image: web-analysis
        imagePullPolicy: Never
        ports:
        - containerPort: 5001
---
apiVersion: v1
kind: Service
metadata:
  name: web-analysis
  namespace: canary-analysis
spec:
  selector:
    app: web-analysis
  ports:
  - protocol: TCP
    port: 5001
    targetPort: 5001
  type: ClusterIP
-----------------------------------------------





ARGO-ROLLOUTS
-------------

|--argo-rollouts
   |-- nginx-ingress.yaml
   |-- nginx-rollouts.yaml
   |-- webhook-ingress.yaml


In nginx-rollouts.yaml CRD we set the type and the steps of the deployment. We will use canaty deployment withe 4 We use also web-analysis as inline analysis.

argo-rollouts/nginx-rollouts.yaml
-------------------------------------------------
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: nginx-rollouts
  namespace: argo-demo
spec:
  revisionHistoryLimit: 5               # Keep only last 5 revisions (default is 10)
  replicas: 5
  strategy:
    canary:
      steps:
      - setWeight: 20                   # first step is 20% of the traffic - 1 of 5 pods
      - pause: {}                       # manual promotion required
      - analysis:                           # This is an Inline Analysis
          templates:
          - templateName: success-rate
            clusterScope: true          # using a cluster-scoped AnalysisTemplate (deployed in another namespace)
          args:
          - name: service_port              # This is the variable related to the port of analysis-app.py flask app
            value: "5001"
      - setWeight: 40                   # second step is 40% of the traffic - 2 of 5 pods
      - pause: {duration: 10s}          # auto promotion afer 10 seconds
      - setWeight: 60                   # third step is 60% of the traffic - 3 of 5 pods
      - pause: {duration: 20s}          # auto promotion afer 20 seconds
      - setWeight: 80                   # fourth step is 80% of the traffic - 4 of 5 pods
      - pause: {duration: 1m}           # auto promotion afer 1 minute
  selector:
    matchLabels:
      app: nginx-rollouts
  template:
    metadata:
      labels:
        app: nginx-rollouts
    spec:
      containers:
      - name: nginx-rollouts
        image: host.docker.internal:8085/argo-demo/nginx:abb6283a-a528-47d3-bbd2-2193447b5a1c
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  ports:
  - port: 8080
    targetPort: 80
  selector:
    app: nginx-rollouts
-------------------------------------------------



We use webhook ingress for GitHub webhooks to reach our EventSource and trigger workflows. Without it, there's no network path from GitHub to your EventSource pod.

argo-rollouts/webhook-ingress.yaml
-----------------------------------------------
# This ingress is matching the Docker internal network with exact path
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argo-events-ingress
  namespace: argo-events
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /push
        pathType: Exact
        backend:
          service:
            name: ci-eventsource-svc
            port:
              number: 12000
-----------------------------------------------


This is the ingress that exposes our application locally and we can access it on http://argo.demo:32073/

argo-rollouts/nginx-ingress.yaml
-------------------------------------------------
# Use if the ingress is connecting out of Docker network
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: argo-demo
spec:
  ingressClassName: nginx
  rules:
  - host: argo.demo
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service
            port:
              number: 8080
-------------------------------------------------


Add host address to Windows host list
	- Open PowerShell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 argo-demo'
		- save the file and exit

Add host address to Linux host list
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 argo-demo'
	- save changes and exit - escape, :wq!, enter

After the rollout is created the app should be accessabel on http://argo.demo:32073/



DEPLOY ARGOCD SETUP
-------------------

Deploy analysis application
---------------------------

1. Build and load analysis appplycation on the cluster
	Navigate to the canary-analysis\analysis-app folder and build the image
		trminal --> docker build -t web-analysis .

	Load the image into the cluster
		terminal --> kind load docker-image web-analysis --name kind


2. Create canary-analysis namespace
	terminal --> k create ns canary-analysis

	# result: namespace/canary-analysis created


3. Deply the analysis CRD in the 'canary-analysis' namespace
	terminal --> k apply -f canary-analysis.yaml

	# result: clusteranalysistemplate.argoproj.io/success-rate created

4. Apply the analysis deployment
	terminal --> k apply -f web-analysis-deployment.yaml -n canary-analysis

	# result:
	deployment.apps/web-analysis created
	service/web-analysis created

5. Check if the serice is running
	terminal --> kubectl get pods -n canary-analysis -l app=web-analysis

	# result:
	NAME                            READY   STATUS    RESTARTS   AGE
	web-analysis-5f9bc64f67-2tgn6   1/1     Running   0          6m29s



GENERATE GITHUB CREDENTIALS
---------------------------

We need to provide GitHub credentials to our workflow to access the application repository and manage it - clone it, make changes to the manifests etc.

Login to GitHub - https://github.com/
Go to https://github.com/settings/tokens
	- Generate New Token (classic)
		- Name: argo-workflows
		- Scope
			- repo - Full control of private repositories 
			- workflow - Update GitHub Action workflows (Optional)
		- Generate Token
	- Copy the value (save it safe temporary)

Create local environment variables with GitHub Username and Token
	terminal --> $GITHUB_USERNAME = "git_username"			# set your username
	terminal --> $GITHUB_TOKEN = "generated_token"			# set the token

Test the creation of the environment cariables
	terminal --> echo $GITHUB_USERNAME
	terminal --> echo $GITHUB_TOKEN


GIVE ARGOCD ACCESS TO GITHUB REPO
---------------------------------

Create secret for GitLab repository access
	terminal --> kubectl create secret generic argocd-github-repo --from-literal=username=$GITHUB_USERNAME --from-literal=password=$GITHUB_TOKEN --from-literal=url=https://github.com/entermix123/argo-config.git -n argocd

Label it so ArgoCD recognizes it as a repository credential
	terminal --> kubectl label secret argocd-github-repo argocd.argoproj.io/secret-type=repository -n argocd

Check if the repository is added successfully in ArgoCD UI - https://argocd.localhost:32074/settings/repos





Create secret for workflow task in the next step in the same terminal session else the envs will be deleted and the access will be denied.

We will use the environment variables to create secrets objects in our Kubernetes cluster and set them in the workflow.

CREATE SECRETS FOR GITHUB CREDENTAILS IN THE SAME SHELL SESSION
---------------------------------------------------------------

1. Generate 20 random bytes and convert to hex string and save it in environemnt variable
	terminal --> $bytes = New-Object byte[] 20
[Security.Cryptography.RNGCryptoServiceProvider]::Create().GetBytes($bytes)
$WEBHOOK_SECRET = ($bytes | ForEach-Object { $_.ToString("x2") }) -join ""

2. Check the env variable
	terminal --> echo $WEBHOOK_SECRET

3. Create webhook secret in 'argo-events' namespace
	terminal --> kubectl create secret generic github-webhook-secret --from-literal=webhook-secret=$WEBHOOK_SECRET -n argo-events

	# result: secret/github-webhook-secret created


4. Create secret in 'argo-events' namespace to use GitHub credentials to access the app repostiory
	terminal --> kubectl create secret generic config-repo-credentials --from-literal=token=$GITHUB_TOKEN --from-literal=username=$GITHUB_USERNAME -n argo-events
	
	# result: secret/config-repo-credentials created


4. Create secret in 'argo-events' namespace to use GitHub credentials to access the argo-config repostiory
	terminal --> kubectl create secret generic app-repo-credentials --from-literal=token=$GITHUB_TOKEN -n argo-events
	
	# result: secret/app-repo-credentials created







DEPLOY ARGO CRDs
----------------

1. Start the ngrok
	terminal --> ngrok http 32073

2. Update event-source.yaml with ngrok URL

event-source.yaml
-------------------------------------------------
...
      webhook:
        endpoint: /push
        port: "12000"
        url: http://$OUR_PUBLIC_DOMAIN 				# set the exposed domain       
...
-------------------------------------------------


Apply the updated manifests
	terminal --> kubectl apply -f event-source.yaml -n argo-events

	# result:
	eventsource.argoproj.io/ci created
	ingress.networking.k8s.io/argo-events-ingress created

Apply the event source CRD
	terminal --> kubectl apply -f sensor.yaml -n argo-events

	# result: sensor.argoproj.io/ci created

Apply the sensor CRD
	terminal --> kubectl apply -f argo-app.yaml

	# result: application.argoproj.io/argo-demo-application created


Check if in the GitHub my-app repository webhook was created.


We can access ArgoCD - https://argocd.localhost:32074/
We can Access Argo Rollouts - http://rollouts.localhost:32073/rollouts/			# set argo-demo namespace
We can access Argo Workflows - https://argo-workflows.localhost:32074/workflows/	# set 'argo-events' namespace
We can access MinIO - http://minio.localhost:32073/browser
We can access Nexus - http://localhost:8081/#browse/browse

After the application is deployed we can access it on http://argo.demo:32073/



MAKE A CHANGE
-------------

When we create and push change to our app

index.html
-------------------------------------------------
    <div class="container">
        <h1>Welcome to My Website, Version 2.0</h1>
    </div>
-------------------------------------------------
# from '1.0' to '2.0' - changed subversion

Commit changes
	terminal --> git add .
	terminal --> git commit -m "version 2"
 	terminal --> git push


The change in the repostiory will trigger the webhook and new workflow will start.

After the workflow finishes the used image will be changed in the rollout.
The application will autoamtic sync after 3 minutes.
The application will start update (canary deployment will start).

We can fooolow the rollout and promote the first step on http://rollouts.localhost:32073/rollouts/ or with Argo UI in the argo-demo namepsace.
	terminal --> kargo promote nginx-rollouts -n argo-demo

	# result: rollout 'nginx-rollouts' promoted

After the rollout is finished and the application is running with the new version we can test it on http://argo.demo:32073/
We should see message 'Welcome to My Website, Version 2.0'




TROUBLESHOOTING COMMANDS
------------------------

Check event source status
	terminal --> kubectl get eventsources -n argo-events
	terminal --> kubectl describe eventsource ci -n argo-events

Check sensor status
	terminal --> kubectl get sensors -n argo-events
	terminal --> kubectl describe sensor ci -n argo-events

Check event source logs
	terminal --> kubectl logs -n argo-events -l eventsource-name=ci

Check sensor logs
	terminal --> kubectl logs -n argo-events -l sensor-name=ci

Manually trigger to test (without webhook)
	terminal --> kubectl create -f - <<EOF
apiVersion: v1
kind: Event
metadata:
  name: test-event
  namespace: argo-events
EOF











24. Install ESO (External Secret Operator) with Helm
====================================================

We will install ESO (External Secret Operator) with Helm on Docker Kind Cluster

Add ESO Helm repository and Update Helm repos
	terminal --> helm repo add external-secrets https://charts.external-secrets.io
	terminal --> helm repo update


Install External Secrets Operator
	terminal --> helm install external-secrets external-secrets/external-secrets -n external-secrets-system --create-namespace --set installCRDs=true


Verify installation
	terminal --> kubectl get pods -n external-secrets-system

	**Expected output:**
	```
	NAME                                                READY   STATUS    RESTARTS   AGE
	external-secrets-xxxxx                              1/1     Running   0          30s
	external-secrets-cert-controller-xxxxx              1/1     Running   0          30s
	external-secrets-webhook-xxxxx                      1/1     Running   0          30s



Configure GitHub Access for ESO
-------------------------------

This is the only manual secret you'll create - it allows ESO to authenticate with GitHub:

Create the bootstrap secret for GitHub authentication
	terminal --> kubectl create secret generic github-token --from-literal=token=YOUR_GITHUB_PERSONAL_ACCESS_TOKEN -n external-secrets-system















