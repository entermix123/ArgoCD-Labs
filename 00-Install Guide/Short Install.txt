Quick Windows Install - ArgoCD

This instruction is for installing cluster modules on Wiondows included in the work of real life CI/CD pipelines
	- Kubernetes Kind Cluster
	- ArgoCD
	- Argo Rollouts
	- Argo Workflows
	- MinIO on the cluster

All prerequisites should be already installed: Docker, Kind, Kubectl, Helm, Nexus, Argo CLI and more from the full instruction - Kubernetes Kind ArgoCD Install Guide for Windows.txt


Cluster
-------

Create cluster 
	terminal --> kind create cluster --config kind-config-nginx.yaml --image kindest/node:v1.34.0

Set roles for worker nodes
	terminal --> kubectl label nodes kind-worker kind-worker2 node-role.kubernetes.io/worker= --overwrite

Rename the local cluster with secret
	terminal --> k apply -f local-secret.yaml


ArgoCD
------

Before installing ArgoCD with nginx ingress controller we need to install nginx deployment
	terminal --> kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.0/deploy/static/provider/kind/deploy.yaml

Edit the deployment and add '--enable-ssl-passthrough' flag in specs
	terminal --> k edit deployment ingress-nginx-controller -n ingress-nginx

------------------------------------------------------------
...
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        - --watch-ingress-without-class=true
        - --enable-metrics=false
        - --publish-status-address=localhost
        - --enable-ssl-passthrough						# added
...
------------------------------------------------------------
save changes

Install ArgoCD with nginx ingress controller
	terminal --> helm install argocd argo/argo-cd -f values-nginx-simplified.yaml -n argocd --create-namespace

Wait a minute and find initial password for ArgoCD:
	shell terminal --> k -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }

	# result: password 

Connect to the Node with ArgoCD CLI
	terminal --> argocd login argocd.localhost:32074 --insecure --grpc-web --username admin --password <password>

Wait a minute and access ArgoCD on https://argocd.localhost:32074



Argo Rollouts
-------------

Create argo-rollouts namespace
	terminal --> kubectl create namespace argo-rollouts

Apply argo-rollouts manifest into argo-rollouts namepsace
	terminal --> kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

Install kargo dashboard
	terminal --> kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/dashboard-install.yaml

Create the service for Argo Rollouts
	terminal --> kubectl apply -f dashboard-ingress.yaml -n argo-rollouts

Open Kargo Dashboard on http://rollouts.localhost:32073/




Argo Workflows
==============

Create installation 'argo' namespace
	terminal --> k create ns argo


HELM:
.....
Deploy Aego Workflows Helm Chart
	terminal --> helm install my-workflow argo/argo-workflows -n argo -f argo-workflows-values.yaml

Create rolebinding for admin role in 'argo' namespace so we can execute actions with Argo Workflows UI
	terminal --> k create rolebinding default-admin --clusterrole=admin --serviceaccount=argo:default -n argo


Manual:
.......
Deploy Argo Workflows manifests
	terminal --> kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/latest/download/install.yaml

Create rolebinding for admin role in 'argo' namespace so we can execute actions with Argo Workflows UI
	terminal --> k create rolebinding default-admin --clusterrole=admin --serviceaccount=argo:default -n argo

Deploy the argo-workflows-ingress.yaml resource
	terminal --> k apply -f argo-workflows-ingress.yaml

Patch the argo workflow server to avoid authentication (NOT FOR PRODUCTION !)
	shell terminal --> kubectl patch deployment argo-server -n argo --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/args", "value": ["server", "--auth-mode=server"]}]'

Wait a minute and access our Argo Worflows UI on https://argo-workflows.localhost:32074/



ARGO EVENTS
-----------

WE NEED TO HAVE INSTALLED ARGO WORKFLOWS TO INSTALL ARGO EVENTS - https://argoproj.github.io/argo-events/quick_start/

Official Installation instructions - https://argoproj.github.io/argo-events/installation/

1. Create argo events namespace
	terminal --> kubectl create namespace argo-events


2. Deploy Argo Events SA, ClusterRoles, and Controller for Sensor, EventBus, and EventSource
	terminal --> kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml
	terminal -> kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml


3. Deploy the eventbus
	terminal --> kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml


4. Create a service account with RBAC settings to allow the sensor to trigger workflows, and allow workflows to function
	Sensor RBAC - allows sensor to trigger workflows
		terminal --> kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/rbac/sensor-rbac.yaml

	Workflow RBAC - allows workflows to function properly
		terminal --> kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/rbac/workflow-rbac.yaml





CONFIGURE ARGO WORKFLOWS PERMISSIONS IN ARGO-EVENTS NAMESPACE 
=============================================================

We have 4 cases
	- case 1 - if Argo Workflows is installed with HELM and we use default namespace service account
	- case 2 - if Argo Workflows is installed with HELM and we use dedicated namespace service account
	- case 3 - if Argo Workflows is installed manually and we use default namespace service account
	- case 4 - if Argo Workflows is installed manually and we use dedicated namespace service account

There is some differences that matters in audit point of view but all cases work. 


IF ARGO WORKFLOWS IN INSTALLED WITH HELM
----------------------------------------

Scenario 1 - Working with the default user account of the working namespace
----------

Give workflow-controller permissions to act with pods in the working namespace:
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:my-workflow-argo-workflows-workflow-controller -n argo-events

Give permissions to the working namespace's default service account:
	terminal --> k create rolebinding default-admin --clusterrole=admin --serviceaccount=argo-events:default -n argo-events



Scenario 2 - Create separate service account for the working namespace
----------

Give workflow-controller permissions to act with pods in the working namespace:
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:my-workflow-argo-workflows-workflow-controller -n argo-events

Create service account in working 'argo-events' namespace
	terminal --> kubectl create serviceaccount argo-workflows -n argo-events

Create rolebinding for the created service account and give it admin rights
	terminal --> kubectl create rolebinding argo-workflows-admin --clusterrole=admin --serviceaccount=argo-events:argo-workflows -n argo-events



IF ARGO WORKFLOWS IN INSTALLED MANUALLY
---------------------------------------

Scenario 1 - Working with the default service account
----------
Give workflow-controller permissions (NOTE: different service account name)
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:argo -n argo-events

Give permissions to working namespace's default service account
	terminal --> kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=argo-events:default -n argo-events


Scenario 2 - Create separate service account
----------
Give workflow-controller permissions (NOTE: different service account name)
	terminal --> kubectl create rolebinding workflow-controller-admin --clusterrole=admin --serviceaccount=argo:argo -n argo-events

Create service account
	terminal --> kubectl create serviceaccount argo-workflows -n argo-events

Give it permissions
	terminal --> kubectl create rolebinding argo-workflows-admin --clusterrole=admin --serviceaccount=argo-events:argo-workflows -n argo-events


NOTES:
------
- The key difference between HELM and Manual is the service account name in 'argo' namespace
  * HELM: argo:my-workflow-argo-workflows-workflow-controller
  * Manual: argo:argo
- Using a dedicated service account (Scenarios 2 & 4) is more secure for production
- For development/testing, default service account (Scenarios 1 & 3) is simpler





MinIO
-----

Install MinIO chart
	terminal --> helm install argo-artifacts minio/minio --set resources.requests.memory=512Mi --set replicas=1 --set persistence.enabled=false --set mode=standalone --set rootUser=admin --set rootPassword=password123 --set buckets[0].name=my-bucket --set buckets[0].policy=none --set buckets[0].purge=false -n argo

Apply ingress manifest
	terminal --> k apply -f minio-ingress.yaml -n argo

Create credentials secret in working namespace
	terminal --> k create secret generic my-minio-cred --from-literal=access-key=admin --from-literal=secret-key=password123 -n argo-events

Create the configmap into the working namespace
	terminal --> kubectl apply -f minio-artifact-repo-cm.yaml -n argo-events

Use MinIO
---------

Find Creadentials:

We can print the secret and see the json paths
	terminal --> k get secret argo-artifacts-minio -n argo -o yaml

Decode the username (rootUser) with shell
	terminal --> kubectl get secret argo-artifacts-minio -n argo -o jsonpath='{.data.rootUser}' | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([Convert]::FromBase64String($_.Trim())) }

	# result: admin

Decode the password (rootPassword) with shell
	terminal --> kubectl get secret argo-artifacts-minio -n argo -o jsonpath='{.data.rootPassword}' | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([Convert]::FromBase64String($_.Trim())) }

	# result: password123

Access the MinIO app on http://minio.localhost:32073/login


Configure all nodes to pull images from nexus
---------------------------------------------
Configure nodes to pull images
	terminal --> kubectl apply -f containerd-config-daemonset.yaml

Wait a minute and check connuctivity
	terminal --> kubectl get nodes

Wait a minute and delete the daemonset
	terminal --> kubectl delete daemonset containerd-registry-config -n kube-system



Proceed with the CI/CD pipeline
===============================

Set the default namespace
	terminal --> k config set-context --current --namespace=argo-events

Check the default namespace
	terminal --> kubectl config get-contexts

	# result:
	CURRENT   NAME            CLUSTER         AUTHINFO        NAMESPACE
	          kind-cluster2   kind-cluster2   kind-cluster2
	*         kind-kind       kind-kind       kind-kind       argo-events	# current namespace



GENERATE GITHUB CREDENTIALS
---------------------------

We need to provide GitHub credentials to our workflow to access the application repository and manage it - clone it, make changes to the manifests etc.

Login to GitHub - https://github.com/
Go to https://github.com/settings/tokens
	- Generate New Token (classic)
		- Name: argo-workflows
		- Scope
			- repo - Full control of private repositories 
			- workflow - Update GitHub Action workflows (Optional)
		- Generate Token
	- Copy the value (save it safe temporary)

Create local environment variables with GitHub Username and Token
	terminal --> $GITHUB_USERNAME = "git_username"			# set your username
	terminal --> $GitHubTokenName = "argo-workflows"		# set the token name
	terminal --> $GITHUB_TOKEN = "generated_token"			# set the token

Test the creation of the environment cariables
	terminal --> echo $GITHUB_USERNAME
	terminal --> echo $GitHubTokenName
	terminal --> echo $GITHUB_TOKEN


Create secretfor workflow task in the next step in the same terminal session else the envs will be deleted and the access will be denied.


We will use the environment variables to create secrets objects in our Kubernetes cluster and set them in the workflow.

CREATE SECRETS FOR GITHUB CREDENTAILS IN THE SAME SHELL SESSION
---------------------------------------------------------------

Create secret in our working namespace to use GitHub credentials safetly
	terminal --> kubectl create secret generic github-credentials --from-literal=username=$GITHUB_USERNAME --from-literal=token_name=$GitHubTokenName --from-literal=token=$GITHUB_TOKEN -n argo-events
	
	# result: secret/github-credentials created




14. Install Nexus private image registry
========================================

1. Download and install Java from: https://www.oracle.com/java/technologies/downloads/#jdk25-windows

	Confirm java installation
		terminal --> java -version

2 Download Nexus Repository OSS (Open Source) for Windows - https://www.sonatype.com/products/nexus-community-edition-download

3. Install Nexus
	- unarchive on the PC and navigate to the folder
	- Open shell as administrator and run
		terminal --> .\install-nexus-service.bat

4. Start Nexus service
	terminal --> net start SonatypeNexusRepository

	Check if port 8081 is listening
		terminal --> netstat -ano | findstr :8081

5. Login to Nexus on http://localhost:8081/#login

	Find the initial generated credentials in the installed directory 'sonatype-work\nexus3\admin.password' or with shell
		terminal --> type E:\Installed\nexus-3.88.0-08-win-x86_64\sonatype-work\nexus3\admin.password

	Login to the app and finish the installtion. Relogin.
		Username: admin
		Password: admin123		(default)


Create Nexus repository - example usage
---------------------------------------
- Open Nexus - http://localhost:8081
- Go to Settings/Repositories/Create Repository/docker(hosted)
	- Name: argo-demo
	- Other Connectors
		- HTTP: 8085
	- Docker Registry API Support
		Select Checkbox "Allow clients to use the V1 API to interact with this repository"
	- Create Repository

- Go to Settings/Security/Realms
	- Set Docker Bearer Token Realm to Active
	- Save

 

Configure Docker Desktop to communicate with the created repository
	- Open Docker/Settings/Docker Engine
	- Add
	-------------------------------------------------
	{
	  "insecure-registries": ["localhost:8085"]
	}
	-------------------------------------------------
	- Apply and Restart



Test Docker connection with Nexus by pushing the image created earlier
	Login to Nesus true the configured address
		terminal --> docker login host.docker.internal:8085
		terminal --> admin
		terminal --> nexsus_password

	Retag (rename) and push the image we created earlier to the Nexus repository
		terminal --> docker tag nginx:v1 host.docker.internal:8085/argo-demo/nginx:alpine
		terminal --> docker push host.docker.internal:8085/argo-demo/nginx:alpine

	The image should be visualized in the Nexus repository
		- Go to http://localhost:8081/#browse/browse
		- Then navigate to v2/argo-demo/nginx/tags




Configure Argo Workflows to pull/push images from/to Nexus
----------------------------------------------------------

We need to allow Argo Workflows communication in the Docker Engine Settings to be able to access Nexus platofrm. In this case we are using Docker Desktop and we add the internal Docker host address and the port of the Nexus repository.

Configure Argo Workflows to communicate with Nexus repository
	- Open Docker/Settings/Docker Engine
	- Add
	-------------------------------------------------
	{
	  "insecure-registries": ["localhost:8085", "host.docker.internal:8085"]
	}
	-------------------------------------------------
	- Apply and Restart



Configure Argo Workflows to pull images from Nexus
--------------------------------------------------


Create containerd daemonset configuration

containerd-config-daemonset.yaml
-------------------------------------------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: containerd-registry-config
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: containerd-registry-config
  template:
    metadata:
      labels:
        name: containerd-registry-config
    spec:
      hostPID: true
      hostNetwork: true
      initContainers:
      - name: configure-containerd
        image: alpine:latest
        command:
        - sh
        - -c
        - |
          set -e
          
          # Check if configuration already exists
          if grep -q "host.docker.internal:8085" /host/etc/containerd/config.toml 2>/dev/null; then
            echo "Registry configuration already exists, skipping..."
            exit 0
          fi
          
          # Append registry configuration
          cat >> /host/etc/containerd/config.toml << 'EOF'

          [plugins."io.containerd.grpc.v1.cri".registry.mirrors."host.docker.internal:8085"]
            endpoint = ["http://host.docker.internal:8085"]
          [plugins."io.containerd.grpc.v1.cri".registry.configs."host.docker.internal:8085".tls]
            insecure_skip_verify = true
          EOF
          
          echo "Configuration added successfully"
          
          # Find and restart containerd process
          CONTAINERD_PID=$(nsenter -t 1 -m -u -i -n -p pgrep containerd | head -n 1)
          if [ -n "$CONTAINERD_PID" ]; then
            echo "Sending SIGHUP to containerd (PID: $CONTAINERD_PID)"
            nsenter -t 1 -m -u -i -n -p kill -HUP $CONTAINERD_PID
            sleep 2
            echo "Containerd reloaded"
          else
            echo "Warning: Could not find containerd process"
          fi
        securityContext:
          privileged: true
        volumeMounts:
        - name: containerd-config
          mountPath: /host/etc/containerd
      containers:
      - name: pause
        image: registry.k8s.io/pause:3.9
        resources:
          requests:
            cpu: 1m
            memory: 4Mi
      volumes:
      - name: containerd-config
        hostPath:
          path: /etc/containerd
          type: Directory
      tolerations:
      - operator: Exists
-------------------------------------------------

Apply the deamonset
	terminal --> kubectl apply -f containerd-config-daemonset.yaml

	# result: daemonset.apps/containerd-registry-config created

Test nodes communication
	terminal --> k get nodes

Wait 2 minutes and delete the daemonset
	terminal --> kubectl delete -f containerd-config-daemonset.yaml



Give Argo Workflows access to Nexus
-----------------------------------

For this task we need to create Docker secret called 'docker-config-secret' in our Argo Workflows working 'argo-events' namespace. We have to mount our docker credentials to this sescret.

We need to create docker-config.json file and set the Nexus credentials so the workflow can pull and push images.

1. Encode the nexus creadentials with shell
	terminal --> $auth = [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes("admin:admin123"))
$auth
	result: YWRtaW46YWRtaW4xMjM=

2. Create docker-config.json
-----------------------------------------
{
  "auths": {
    "host.docker.internal:8085": {
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
-----------------------------------------

From the folder location create Docker secret
	terminal --> kubectl create secret generic docker-config-secret --from-file=config.json=./docker-config.json -n argo-events

	# result: secret/docker-config-secret created





GENERATE GITHUB CREDENTIALS
---------------------------

We need to provide GitHub credentials to our workflow to access the application repository and manage it - clone it, make changes to the manifests etc.

Login to GitHub - https://github.com/
Go to https://github.com/settings/tokens
	- Generate New Token (classic)
		- Name: argo-workflows
		- Scope
			- repo - Full control of private repositories 
			- workflow - Update GitHub Action workflows (Optional)
		- Generate Token
	- Copy the value (save it safe temporary)

Create local environment variables with GitHub Username and Token
	terminal --> $GITHUB_USERNAME = "git_username"			# set your username
	terminal --> $GitHubTokenName = "argo-workflows"		# set the token name
	terminal --> $GITHUB_TOKEN = "generated_token"			# set the token

Test the creation of the environment cariables
	terminal --> echo $GITHUB_USERNAME
	terminal --> echo $GitHubTokenName
	terminal --> echo $GITHUB_TOKEN


Create secretfor workflow task in the next step in the same terminal session else the envs will be deleted and the access will be denied.


We will use the environment variables to create secrets objects in our Kubernetes cluster and set them in the workflow.

CREATE SECRETS FOR GITHUB CREDENTAILS IN THE SAME SHELL SESSION
---------------------------------------------------------------

Create secret in our working namespace to use GitHub credentials safetly
	terminal --> kubectl create secret generic github-credentials --from-literal=username=$GITHUB_USERNAME --from-literal=token_name=$GitHubTokenName --from-literal=token=$GITHUB_TOKEN -n argo-events
	
	# result: secret/github-credentials created





Install local GitLab repository with Docker 
-------------------------------------------

Create volumes on your Windows PC
	terminal --> mkdir D:\Docker\gitlab\config
	terminal --> mkdir D:\Docker\gitlab\logs
	terminal --> mkdir D:\Docker\gitlab\data

Start Docker container with GitLab with exact version (if we use latest we can update the version and lose the configs)
	terminal --> docker run -d --hostname gitlab.local --name gitlab -p 80:80 -p 443:443 -p 22:22 --restart always -v D:\Docker\gitlab\config:/etc/gitlab -v D:\Docker\gitlab\logs:/var/log/gitlab -v D:\Docker\gitlab\data:/var/opt/gitlab gitlab/gitlab-ce:18.8.1-ce.0


Set new password
----------------

Connecto the gitlab container
	terminal --> docker exec -it gitlab bash

Run the GitLab Rails console
		terminal --> gitlab-rails console -e production

Set the new password adn save it
		terminal --> user = User.find_by(username: 'root')
		terminal --> user.password = 'YourStrongPassword123!'
		terminal --> user.password_confirmation = 'YourStrongPassword123!'
		terminal --> user.save!

		# result: => true

Exit the console and container
	terminal --> exit
	terminal --> exit

One line shell command (change the password)
	terminal --> docker exec -it gitlab gitlab-rails runner "user = User.find_by(username: 'root'); user.password = 'YourStrongPassword123!'; user.password_confirmation = 'YourStrongPassword123!'; user.save!"

	
Add host address to Windows host list on Windows
	- Open power Shell as Admin
		terminal --> notepad C:\Windows\System32\drivers\etc\hosts
		- add '127.0.0.1 gitlab.local'
		- save the file and exit

Add host address to Windows host list on Linux
	terminal --> sudo vim /etc/hosts
	- Add '127.0.0.1 gitlab.local'
	- save changes and exit - escape, :wq!, enter



Login to GitLab on http://gitlab.localhost/users/sign_in
	Username: root
	Password: your_password

Create a blank project
	- name: argo-config
	- Project URL: http://gitlab.localhost/root/argo-config
	- Create project

Creaet another project for our app
	- name: my-app
	- Project URL: http://gitlab.localhost/root/my-app
	- Create project



TEST CLUSTER <--> GITLAB CONNECTION
-----------------------------------

Our cluster must be connected with our GitLab server. In this case we are running both via Docker containers

Find our GitLab IP address
	terminal --> docker network inspect bridge | Select-String "Gateway"

	# result: 172.17.0.1

Test connections between cluster and GitLab
	Try to reach Kind cluster from GitLab container
		terminal --> docker exec -it gitlab curl -v http://172.17.0.1:32073
	Try to reach GitLab container from Kind cluster
		terminal --> docker exec -it kind-control-plane curl -v http://172.17.0.1

	# if successfull we will set this IPs for connection between gitlab and kind cluster


GENERATE GITLAB ACCESS TOKEN
----------------------------

Create User Personal Token to connect the 'argo-config' repository to ArgoCD
	- connect to GitLab container
		terminal --> docker exec -it gitlab gitlab-rails console

	- Find the admin user 
		terminal --> user = User.find_by(username: 'root')


	- create personal token
		terminal --> 
token = user.personal_access_tokens.create(
  name: 'gitlab-full-access',
  scopes: ['api', 'read_repository', 'write_repository'],
  expires_at: 1.year.from_now
)
	
	- Display the token - COPYAND SAVE THE TOKEN IMMEDIATELY!
		terminal --> 
puts "=" * 60
puts "TOKEN: #{token.token}"
puts "=" * 60

	- Verify creation
		terminal --> 
if token.persisted?
  puts "✅ Token created successfully!"
  puts "Name: #{token.name}"
  puts "Scopes: #{token.scopes}"
else
  puts "❌ Error: #{token.errors.full_messages}"
end

	- Exit
		terminal --> exit	


Create access token for argo-config project
	- go tp my-app repo/ Settings/Access tokens/Add new token
		- Token name: argo-config-token
		- Expiration date: No Expiration Date
		- Select a role: Maintainer
			- check 'api', 'read_repository' and 'write_repository' option
		- Create project access token

Create access token for my-app project
	- go tp my-app repo/ Settings/Access tokens/Add new token
		- Token name: my-app-token
		- Expiration date: No Expiration Date
		- Select a role: Maintainer
			- check 'api', 'read_repository' and 'write_repository' option
		- Create project access token

We can use different tokens for the different projects (my-app and argo-config) We can use one user access token - not a good practice.

Since the user is the same we use one env var in both secrets.


ALLOW LOCAL HOOKS
-----------------
Go to GitLab/Admin/Settings/Network/Outbound requests/
	- check 'Allow requests to the local network from webhooks and integrations'
	- check 'Allow requests to the local network from system hooks'
	- in the Local IP addresses and domain names ... add
		172.17.0.1
		127.0.0.1
		argo.events
	- Save Changes



CREATE ENV VARIABLES WITH GITLAB CREDENTIALS 
--------------------------------------------
Create local environment variables with GitLab Username and Tokens
	terminal --> $GITLAB_USERNAME = "root"				# set your username
	terminal --> $GITLAB_USER_TOKEN = "generated_user_token"	# set your user token
	terminal --> $GITLAB_CONFIG_TOKEN = "generated_config_token"	# set the config token
	terminal --> $GITLAB_APP_TOKEN = "generated_app_token"		# set the app token

Test the creation of the environment cariables
	terminal --> echo $GITLAB_USERNAME
	terminal --> echo $GITLAB_USER_TOKEN
	terminal --> echo $GITLAB_APP_TOKEN
	terminal --> echo $GITLAB_CONFIG_TOKEN


CREATE SECRETS FOR GITLAB CREDENTAILS
-------------------------------------
Create secret in 'argo-events' namespace to use GitLab credentials for creating the webhook in my-app project
	terminal --> kubectl create secret generic app-repo-credentials --from-literal=username=$GITLAB_USERNAME --from-literal=token=$GITLAB_APP_TOKEN -n argo-events

	# result: secret/app-repo-credentials created

Create secret in 'argo-events' namespace to use GitLab credentials for managing used image in the roolout in argo-config project
	terminal --> kubectl create secret generic config-repo-credentials --from-literal=username=$GITLAB_USERNAME --from-literal=token=$GITLAB_CONFIG_TOKEN -n argo-events

	# result: secret/github-credentials created

We use this secret in event source and in the sersor manifests.



GIVE ARGOCD ACCESS TO GITLAB REPO
---------------------------------

Create secret for GitLab repository access
	terminal --> kubectl create secret generic argocd-gitlab-repo --from-literal=username=root --from-literal=password=$GITLAB_USER_TOKEN --from-literal=url=http://172.17.0.1/root/argo-config.git -n argocd

Label it so ArgoCD recognizes it as a repository credential
	terminal --> kubectl label secret argocd-gitlab-repo argocd.argoproj.io/secret-type=repository -n argocd

Check if the repository is added successfully in ArgoCD UI - https://argocd.localhost:32074/settings/repos



GIVE ARGO WORKFLOWS ACCESS TO NEXUS
-----------------------------------

For this task we need to create Docker secret called 'docker-config-secret' in our Argo Workflows working 'argo-events' namespace. We have to mount our docker credentials to this sescret.

We need to create docker-config.json file and set the Nexus credentials so the workflow can pull and push images.

1. Encode the nexus creadentials with shell
	terminal --> $auth = [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes("admin:admin123"))
$auth
	result: YWRtaW46YWRtaW4xMjM=

2. Create docker-config.json
-----------------------------------------
{
  "auths": {
    "host.docker.internal:8085": {
      "auth": "YWRtaW46YWRtaW4xMjM="
    }
  }
}
-----------------------------------------

From the folder location create Docker secret
	terminal --> kubectl create secret generic docker-config-secret --from-file=config.json=./docker-config.json -n argo-events

	# result: secret/docker-config-secret created




22. INSTALL NGROK
=================
We will expose our PC with public domain with ngrok. Download ngrok - https://dashboard.ngrok.com/get-started

Download ngrok for Windows 64bit and extract it in C:\Users\your_user\ngrok.exe

Set Path to the Environment Variables - open power shell as administrator
	admin terminal --> $oldPath = [Environment]::GetEnvironmentVariable("Path", "User")
	admin terminal --> [Environment]::SetEnvironmentVariable("Path", "$oldPath;C:\Users\your-user", "User")   # change user

Test ngrok installation
	terminal --> ngrok version
	terminal --> Get-Command ngrok

	# result: ngrok version 3.35.0
	# result: Application     ngrok  3.35.0     C:\Users\your-user\ngrok.exe
	

Run the following command to add your authtoken to the default ngrok.yml configuration file.
	terminal --> ngrok config add-authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

We will use this command later to expose our PC with the command
	terminal --> ngrok http 32073					# USE LATER !!!

Copy the address shown in the next section to set it in the Argo event-source.yaml manifest.

To prevent usage of the domain in by external requests we will set additional webhook secret.







Deploy analysis application
---------------------------

1. Build and load analysis appplycation on the cluster
	Navigate to the canary-analysis\analysis-app folder and build the image
		trminal --> docker build -t web-analysis .

	Load the image into the cluster
		terminal --> kind load docker-image web-analysis --name kind


2. Create canary-analysis namespace
	terminal --> k create ns canary-analysis

	# result: namespace/canary-analysis created


3. Deply the analysis CRD in the 'canary-analysis' namespace
	terminal --> k apply -f canary-analysis.yaml

	# result: clusteranalysistemplate.argoproj.io/success-rate created

4. Apply the analysis deployment
	terminal --> k apply -f web-analysis-deployment.yaml -n canary-analysis

	# result:
	deployment.apps/web-analysis created
	service/web-analysis created

5. Check if the serice is running
	terminal --> kubectl get pods -n canary-analysis -l app=web-analysis

	# result:
	NAME                            READY   STATUS    RESTARTS   AGE
	web-analysis-5f9bc64f67-2tgn6   1/1     Running   0          6m29s





Install ESO (External Secret Operator) with Helm
================================================

We will install ESO (External Secret Operator) with Helm on Docker Kind Cluster

Add ESO Helm repository and Update Helm repos
	terminal --> helm repo add external-secrets https://charts.external-secrets.io

	# result: "external-secrets" has been added to your repositories

	terminal --> helm repo update

	# result: Update Complete. ⎈Happy Helming!⎈


Install External Secrets Operator
	terminal --> helm install external-secrets external-secrets/external-secrets -n external-secrets-system --create-namespace --set installCRDs=true --set crds.createClusterExternalSecret=true --set crds.createClusterSecretStore=true --set crds.createPushSecret=true

Wait 30 seconds and confirm pods to be ready
	terminal --> kubectl get pods -n external-secrets-system

	**Expected output:**
	```
	NAME                                                READY   STATUS    RESTARTS   AGE
	external-secrets-xxxxx                              1/1     Running   0          30s
	external-secrets-cert-controller-xxxxx              1/1     Running   0          30s
	external-secrets-webhook-xxxxx                      1/1     Running   0          30s



Create GitHub Personal Access Token
-----------------------------------
1. Go to GitHub → Settings → Developer settings → Personal access tokens → Tokens (classic)
2. Click "Generate new token (classic)"
3. Configure:
	- Note: ESO Access Token
	- Expiration: 1 year (or no expiration for testing)
	- Scopes: Check repo (full control of private repositories)
	- Generate token

Create a secret with the GitHub token for ESO to authenticate
	terminal --> kubectl create secret generic github-eso-auth --from-literal=token=ghp_your_eso_token_here -n argo-events


Also create in argocd namespace
	terminal --> kubectl create secret generic github-eso-auth --from-literal=token=ghp_your_eso_token_here -n argocd



SET ESO MANAGED SECRETS
-----------------------

We need to set secret for with local Nexus registry creadentials so the workflow can pull and push images.

1. Encode Nexus user and password with shell - example credentials
	terminal --> $auth = [Convert]::ToBase64String([Text.Encoding]::UTF8.GetBytes("admin:admin123"))
$auth
	result: YWRtaW46YWRtaW4xMjM=

2. Set docker cofig in json format where we set our Nexus registry url and encoded credrntials:
	- {"auths":{"host.docker.internal:8085":{"auth":"YWRtaW46YWRtaW4xMjM="}}}


3. Generate 20 random bytes and convert to hex string and save it in environemnt variable
	terminal --> $bytes = New-Object byte[] 20
[Security.Cryptography.RNGCryptoServiceProvider]::Create().GetBytes($bytes)
$WEBHOOK_SECRET = ($bytes | ForEach-Object { $_.ToString("x2") }) -join ""
echo $WEBHOOK_SECRET

	# result: xxxxxxxxxxxxxxxxxxxxxxxxxxxxx



Install OpenBao Secret Management Server
==============================================


Create volume folders
	terminal --> New-Item -Path "D:\repos\OpenBao\data" -ItemType Directory -Force
	terminal --> New-Item -Path "D:\repos\OpenBao\config" -ItemType Directory -Force

Create the config file
	shell terminal --> 
@"
storage "file" {
  path = "/openbao/data"
}
listener "tcp" {
  address     = "0.0.0.0:8200"
  tls_disable = 1
}
api_addr = "http://0.0.0.0:8200"
ui = true
"@ | Out-File -FilePath D:\repos\OpenBao\config\openbao-config.hcl -Encoding ASCII


Production config with HTTPS
-------------------------------------------------------
storage "file" {
  path = "/openbao/data"
}

listener "tcp" {
  address       = "0.0.0.0:8200"
  tls_disable   = 0  # Enable TLS
  tls_cert_file = "/openbao/config/cert.pem"
  tls_key_file  = "/openbao/config/key.pem"
}

api_addr = "https://your-domain.com:8200"
ui = true
-------------------------------------------------------


Start OpenBao container
	shell terminal --> docker run -d --name openbao -p 8200:8200 -e BAO_ADDR='http://127.0.0.1:8200' -v D:\repos\OpenBao\data:/openbao/data -v D:\repos\OpenBao\config:/openbao/config --cap-add=IPC_LOCK quay.io/openbao/openbao:latest server "-config=/openbao/config/openbao-config.hcl"


Initialize OpenBao
	terminal --> docker exec -it openbao bao operator init

	# result:
	Unseal Key 1: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
	Unseal Key 2: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
	Unseal Key 3: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
	Unseal Key 4: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
	Unseal Key 5: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

	Initial Root Token: xxxxxxxxxxxxxxxxxxxxxxxxxx			# copy this token


Login to http://localhost:8200/
	- Login every time we restart the container !!!
	- add Unseal Key 1, Unseal Key 2, Unseal Key 3 and Initial Root Token


Enable the KV secrets engine in OpenBao
---------------------------------------

Login to OpenBao
	terminal --> docker exec -it openbao bao login

Enable KV v2 secrets engine
	terminal --> docker exec -it openbao bao secrets enable -path=secret kv-v2


⚠️ IMPORTANT: Every time you restart the OpenBao container, you must unseal it again:
	terminal --> docker exec -it openbao bao operator unseal <key1>
	terminal --> docker exec -it openbao bao operator unseal <key2>
	terminal --> docker exec -it openbao bao operator unseal <key3>

Then you can login with your root token at http://localhost:8200



Store all your secrets in OpenBao
	terminal --> docker exec -it openbao bao kv put secret/github-creds USERNAME_GITHUB=entermix123
USER_TOKEN_GITHUB=ghp_your_pat_token APP_TOKEN_GITHUB=ghp_your_pat_token CONFIG_TOKEN_GITHUB=ghp_your_pat_token REPO_URL_GITHUB=https://github.com/entermix123/argo-config.git DOCKER_CONFIG_JSON="{\`"auths\`":{\`"host.docker.internal:8085\`":{\`"auth\`":\`"YWRtaW46YWRtaW4xMjM=\`"}}}" WEBHOOK_SECRET_GITHUB=your_webhook_secret MINIO_ACCESS_KEY=admin MINIO_SECRET_KEY=MinIOPass

⚠️ Add your tokens and secrets



Create the policy directly in the container using a here-document
	terminal --> 
@"
path "secret/data/github-creds" {
  capabilities = ["read"]
}
"@ | Out-File -FilePath "$env:TEMP\eso-policy.hcl" -Encoding ASCII -NoNewline


Copy it to the container
	terminal --> docker cp "$env:TEMP\eso-policy.hcl" openbao:/tmp/eso-policy.hcl

	# result: Successfully copied 2.05kB to openbao:/tmp/eso-policy.hcl

Apply the policy
	terminal --> docker exec -it openbao bao policy write eso-policy /tmp/eso-policy.hcl

	# result: Success! Uploaded policy: eso-policy


Enable AppRole authentication and create a role for ESO
	terminal --> docker exec -it openbao bao auth enable approle

	# result: Success! Enabled approle auth method at: approle/

Create AppRole for ESO
	terminal --> docker exec -it openbao bao write auth/approle/role/eso token_policies=eso-policy token_ttl=1h token_max_ttl=4h

	# result: Success! Data written to: auth/approle/role/eso

Get Role ID
	terminal --> docker exec -it openbao bao read auth/approle/role/eso/role-id

	# result: 
	Key        Value
	---        -----
	role_id    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx				# SAVE ROLE ID


Get Secret ID
	terminal --> docker exec -it openbao bao write -f auth/approle/role/eso/secret-id

	# result:
	Key                   Value
	---                   -----
	secret_id             xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx		# SAVE SECRET ID
	secret_id_accessor    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
	secret_id_num_uses    0
	secret_id_ttl         0s


Create Kubernetes secrets with OpenBao credentials
	terminal --> kubectl create secret generic openbao-approle --from-literal=role-id=<your-role-id> --from-literal=secret-id=<your-secret-id> -n argo-events

	terminal --> kubectl create secret generic openbao-approle --from-literal=role-id=<your-role-id> --from-literal=secret-id=<your-secret-id> -n argocd



Troubleshooting
---------------

If ExternalSecrets show "SecretSyncedError":
1. Check SecretStore status:
	terminal --> kubectl describe secretstore openbao-secret-store -n argo-events

2. Check OpenBao is unsealed:
	terminal --> docker exec -it openbao bao status

3. Force ExternalSecret refresh:
	terminal --> kubectl delete externalsecret <name> -n <namespace>
	terminal --> kubectl apply -f external-secrets/<file>.yaml

4. Check ESO operator logs:
	terminal --> kubectl logs -n external-secrets-system -l app.kubernetes.io/name=external-secrets --tail=50














DEPLOY ARGO CRDs IF NOT AND TEST THE SETUP
------------------------------------------

1. Start the ngrok
	terminal --> ngrok http 32073

2. Update event-source.yaml with ngrok URL

event-source.yaml
-------------------------------------------------
...
      webhook:
        endpoint: /push
        port: "12000"
        url: http://$OUR_PUBLIC_DOMAIN 				# set the exposed domain       
...
-------------------------------------------------

Apply the updated manifests
	terminal --> kubectl apply -f event-source.yaml -n argo-events

	# result:
	eventsource.argoproj.io/ci created
	ingress.networking.k8s.io/argo-events-ingress created

Apply the event source CRD
	terminal --> kubectl apply -f sensor.yaml -n argo-events

	# result: sensor.argoproj.io/ci created

Apply the sensor CRD
	terminal --> kubectl apply -f argo-app.yaml

	# result: application.argoproj.io/argo-demo-application created


Check if in the GitHub my-app repository webhook was created.


We can access ArgoCD - https://argocd.localhost:32074/
We can Access Argo Rollouts - http://rollouts.localhost:32073/rollouts/			# set argo-demo namespace
We can access Argo Workflows - https://argo-workflows.localhost:32074/workflows/	# set 'argo-events' namespace
We can access MinIO - http://minio.localhost:32073/browser
We can access Nexus - http://localhost:8081/#browse/browse
We can access OpenBao - http://localhost:8200/

After the application is deployed we can access it on http://argo.demo:32073/





Backup Docker Desktop Kind clustr
=====================================


IMPORTANT: Your cluster uses GitOps with ArgoCD, so most resources are in Git.
This backup focuses on: cluster state, secrets, and configuration not in Git.


Prerequisites
-------------
Your Git repositories should be backed up separately:
- argo-config repository (all ArgoCD manifests)
- my-app repository (application code)


Part 1: Backup Git Repositories
================================

Backup your Git repos (most important!)
	terminal --> cd D:\repos
	terminal --> git clone https://github.com/entermix123/argo-config.git argo-config-backup
	terminal --> git clone https://github.com/entermix123/my-app.git my-app-backup

Or create archives
	terminal --> Compress-Archive -Path D:\repos\argo-config -DestinationPath D:\backups\argo-config-$(Get-Date -Format 'yyyy-MM-dd').zip
	terminal --> Compress-Archive -Path D:\repos\my-app -DestinationPath D:\backups\my-app-$(Get-Date -Format 'yyyy-MM-dd').zip


Part 2: Backup Kind Cluster Configuration
==========================================

Save Kind cluster config
	terminal --> kind get clusters
	# result: kind

Export kubeconfig
	terminal --> kind export kubeconfig --name kind
	# saved at C:\Users\your-user\.kube\config
	terminal --> Copy-Item C:\Users\$env:USERNAME\.kube\config D:\backups\kubeconfig-$(Get-Date -Format 'yyyy-MM-dd').yaml


Part 3: Backup Critical Secrets and State
==========================================

Create backup directory
	terminal --> New-Item -Path "D:\backups\cluster-backup-$(Get-Date -Format 'yyyy-MM-dd')" -ItemType Directory -Force
	terminal --> cd D:\backups\cluster-backup-$(Get-Date -Format 'yyyy-MM-dd')

1. Backup OpenBao data and config
	terminal --> Copy-Item -Path D:\repos\OpenBao -Destination .\OpenBao -Recurse
	
	# Also backup OpenBao unseal keys and root token (CRITICAL!)
	# Store these in a secure location (password manager, encrypted file)

2. Backup ArgoCD initial admin password
	terminal --> kubectl get secret argocd-initial-admin-secret -n argocd -o yaml > argocd-admin-secret.yaml

3. Backup ArgoCD applications state
	terminal --> kubectl get applications -n argocd -o yaml > argocd-applications.yaml

4. Backup namespaces
	terminal --> kubectl get namespaces -o yaml > namespaces.yaml

5. Backup PersistentVolumes and PersistentVolumeClaims
	terminal --> kubectl get pv,pvc --all-namespaces -o yaml > persistent-volumes.yaml

6. Backup custom CRDs (if any not managed by Helm)
	terminal --> kubectl get crd -o yaml > custom-crds.yaml

7. Backup Ingress configurations
	terminal --> kubectl get ingress --all-namespaces -o yaml > ingress.yaml

8. Backup ESO/OpenBao AppRole secrets (CRITICAL!)
	terminal --> kubectl get secret openbao-approle -n argo-events -o yaml > openbao-approle-argo-events.yaml
	terminal --> kubectl get secret openbao-approle -n argocd -o yaml > openbao-approle-argocd.yaml

9. Backup any secrets NOT managed by ESO
	terminal --> kubectl get secrets --all-namespaces -o yaml > all-secrets.yaml
	
	# WARNING: This contains sensitive data. Encrypt or store securely!


Part 4: Backup Helm Releases
=============================

List all Helm releases
	terminal --> helm list --all-namespaces

Export Helm release values
	terminal --> helm get values argocd -n argocd > helm-argocd-values.yaml
	terminal --> helm get values external-secrets -n external-secrets-system > helm-eso-values.yaml
	terminal --> helm get values argo-events -n argo-events > helm-argo-events-values.yaml
	terminal --> helm get values argo-workflows -n argo-events > helm-argo-workflows-values.yaml
	terminal --> helm get values argo-rollouts -n argo-rollouts > helm-argo-rollouts-values.yaml


Part 5: Document Current State
===============================

Document installed versions
	terminal --> @"
# Cluster State - $(Get-Date -Format 'yyyy-MM-dd HH:mm')

## Helm Charts
$(helm list --all-namespaces | Out-String)

## Docker Images
$(kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.spec.containers[*].image}{"\n"}{end}' | Sort-Object -Unique | Out-String)

## Nodes
$(kubectl get nodes -o wide | Out-String)

## ArgoCD Apps
$(kubectl get applications -n argocd | Out-String)

## ExternalSecrets
$(kubectl get externalsecrets -A | Out-String)
"@ | Out-File cluster-state.txt


Part 6: Create Complete Backup Archive
=======================================

	terminal --> $backupDate = Get-Date -Format 'yyyy-MM-dd'
	terminal --> Compress-Archive -Path "D:\backups\cluster-backup-$backupDate\*" -DestinationPath "D:\backups\complete-backup-$backupDate.zip"

	# Result: All critical data in one archive


===============================================================================
RESTORE PROCESS
===============================================================================

Part 1: Prerequisites
=====================

1. Ensure Git repositories are available
2. Have OpenBao unseal keys and root token ready
3. Have backup files accessible


Part 2: Recreate Kind Cluster
==============================

Delete old cluster (if exists)
	terminal --> kind delete cluster --name kind

Create new cluster with same config
	terminal --> kind create cluster --config D:\repos\argo-demo-github\kind-config.yaml

Verify cluster
	terminal --> kubectl cluster-info
	terminal --> kubectl get nodes


Part 3: Install Core Components (in order)
===========================================

1. Install ArgoCD
	terminal --> helm repo add argo https://argoproj.github.io/argo-helm
	terminal --> helm repo update
	terminal --> helm install argocd argo/argo-cd -n argocd --create-namespace -f helm-argocd-values.yaml

	Wait for ArgoCD to be ready
	terminal --> kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

2. Restore ArgoCD admin password (optional)
	terminal --> kubectl apply -f argocd-admin-secret.yaml

3. Install External Secrets Operator
	terminal --> helm repo add external-secrets https://charts.external-secrets.io
	terminal --> helm install external-secrets external-secrets/external-secrets -n external-secrets-system --create-namespace --set installCRDs=true

4. Install Argo Events
	terminal --> helm install argo-events argo/argo-events -n argo-events --create-namespace -f helm-argo-events-values.yaml

5. Install Argo Workflows
	terminal --> helm install argo-workflows argo/argo-workflows -n argo-events -f helm-argo-workflows-values.yaml

6. Install Argo Rollouts
	terminal --> helm install argo-rollouts argo/argo-rollouts -n argo-rollouts --create-namespace -f helm-argo-rollouts-values.yaml


Part 4: Restore OpenBao
=======================

1. Restore OpenBao data and config
	terminal --> Copy-Item -Path "D:\backups\cluster-backup-$backupDate\OpenBao\*" -Destination D:\repos\OpenBao -Recurse -Force

2. Start OpenBao container
	terminal --> docker run -d --name openbao -p 8200:8200 -e BAO_ADDR='http://127.0.0.1:8200' -v D:\repos\OpenBao\data:/openbao/data -v D:\repos\OpenBao\config:/openbao/config --cap-add=IPC_LOCK quay.io/openbao/openbao:latest server "-config=/openbao/config/openbao-config.hcl"

3. Unseal OpenBao with your saved keys
	terminal --> docker exec -it openbao bao operator unseal <unseal-key-1>
	terminal --> docker exec -it openbao bao operator unseal <unseal-key-2>
	terminal --> docker exec -it openbao bao operator unseal <unseal-key-3>

4. Verify OpenBao data
	terminal --> docker exec -it openbao bao login
	# Enter root token
	terminal --> docker exec -it openbao bao kv get secret/github-creds


Part 5: Restore ESO Configuration
==================================

1. Restore OpenBao AppRole secrets
	terminal --> kubectl apply -f openbao-approle-argo-events.yaml
	terminal --> kubectl apply -f openbao-approle-argocd.yaml

2. Apply ESO configuration from Git
	terminal --> cd D:\repos\argo-config
	terminal --> kubectl apply -f eso-config/secret-store.yaml
	terminal --> kubectl apply -f eso-config/external-secrets/

3. Verify ExternalSecrets are syncing
	terminal --> kubectl get externalsecrets -A
	# All should show "SecretSynced" and "Ready: True"


Part 6: Restore ArgoCD Applications
====================================

ArgoCD applications will be recreated from Git automatically!

1. Apply ArgoCD Application manifests
	terminal --> cd D:\repos\argo-config
	terminal --> kubectl apply -f argo-cd/

2. Wait for ArgoCD to sync
	terminal --> kubectl get applications -n argocd
	
3. Access ArgoCD UI and sync applications if needed
	terminal --> https://argocd.localhost:32074/


Part 7: Restore Argo Events Pipeline
=====================================

1. Apply Event Source and Sensor
	terminal --> cd D:\repos\argo-config
	terminal --> kubectl apply -f argo-events/

2. Verify webhook ingress
	terminal --> kubectl get ingress -n argo-events


Part 8: Verification
====================

1. Check all namespaces
	terminal --> kubectl get namespaces

2. Check all pods are running
	terminal --> kubectl get pods --all-namespaces

3. Check ArgoCD applications
	terminal --> kubectl get applications -n argocd

4. Check ExternalSecrets
	terminal --> kubectl get externalsecrets -A

5. Check ingresses
	terminal --> kubectl get ingress --all-namespaces

6. Test access to UIs:
	- ArgoCD: https://argocd.localhost:32074/
	- Argo Rollouts: http://rollouts.localhost:32073/
	- Argo Workflows: https://argo-workflows.localhost:32074/
	- OpenBao: http://localhost:8200/


===============================================================================
CRITICAL FILES TO KEEP SAFE
===============================================================================

Store these in a secure, encrypted location:

1. OpenBao unseal keys (5 keys) - CRITICAL!
2. OpenBao root token - CRITICAL!
3. OpenBao AppRole role-id and secret-id
4. ArgoCD admin password
5. GitHub PAT tokens
6. Nexus credentials
7. Complete backup archive: complete-backup-YYYY-MM-DD.zip


===============================================================================
BEST PRACTICES
===============================================================================

1. **Daily backups**: Automate with a PowerShell script
2. **Version control**: Keep argo-config and my-app repos up to date
3. **Test restores**: Regularly test the restore process
4. **Multiple locations**: Store backups in multiple locations
5. **Encryption**: Encrypt backup archives containing secrets
6. **Documentation**: Update this document when cluster changes


Automated Backup Script (Optional)
===================================

Create: D:\scripts\backup-cluster.ps1
-------------------------------------------------
# Automated Cluster Backup Script
$backupDate = Get-Date -Format 'yyyy-MM-dd'
$backupPath = "D:\backups\cluster-backup-$backupDate"

# Create backup directory
New-Item -Path $backupPath -ItemType Directory -Force
cd $backupPath

# Backup Git repos
git clone https://github.com/entermix123/argo-config.git
git clone https://github.com/entermix123/my-app.git

# Backup OpenBao
Copy-Item -Path D:\repos\OpenBao -Destination .\OpenBao -Recurse

# Backup K8s resources
kubectl get secret argocd-initial-admin-secret -n argocd -o yaml > argocd-admin-secret.yaml
kubectl get applications -n argocd -o yaml > argocd-applications.yaml
kubectl get secret openbao-approle -n argo-events -o yaml > openbao-approle-argo-events.yaml
kubectl get secret openbao-approle -n argocd -o yaml > openbao-approle-argocd.yaml
kubectl get namespaces -o yaml > namespaces.yaml
kubectl get pv,pvc --all-namespaces -o yaml > persistent-volumes.yaml
kubectl get ingress --all-namespaces -o yaml > ingress.yaml

# Backup Helm values
helm get values argocd -n argocd > helm-argocd-values.yaml
helm get values external-secrets -n external-secrets-system > helm-eso-values.yaml
helm get values argo-events -n argo-events > helm-argo-events-values.yaml
helm get values argo-workflows -n argo-events > helm-argo-workflows-values.yaml
helm get values argo-rollouts -n argo-rollouts > helm-argo-rollouts-values.yaml

# Create archive
cd D:\backups
Compress-Archive -Path "cluster-backup-$backupDate\*" -DestinationPath "complete-backup-$backupDate.zip"

# Clean up old backups (keep last 7 days)
Get-ChildItem -Path D:\backups -Filter "*.zip" | Where-Object { $_.CreationTime -lt (Get-Date).AddDays(-7) } | Remove-Item

Write-Host "Backup completed: complete-backup-$backupDate.zip"
-------------------------------------------------

Schedule this script to run daily:
	- Windows Task Scheduler
	- Run: powershell.exe -ExecutionPolicy Bypass -File D:\scripts\backup-cluster.ps1




